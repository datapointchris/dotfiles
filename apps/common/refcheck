#!/usr/bin/env python3
"""
refcheck - Find broken file references and fragile path patterns

A fast reference validator for codebases. Catches broken source statements,
missing script references, old path patterns, and fragile paths that break
when run from different directories or after file moves.

Designed for:
  - Proactive error detection before running tests
  - Refactoring safety (find stale references after moving files)
  - Path fragility detection (warn about working-directory-dependent paths)
  - Fast feedback (runs in seconds vs minutes for full test suites)

Common workflows:
  refcheck                           # Validate all references + warn about fragile paths
  refcheck --no-warn                 # Only check for errors, skip warnings
  refcheck --strict                  # Treat warnings as errors (CI mode)
  refcheck management/               # Check specific directory
  refcheck --pattern "old/" --desc "Now new/"  # After refactoring
  refcheck --type sh --skip-docs     # Only shell scripts, skip docs
"""

import re
import sys
import subprocess
from pathlib import Path
from typing import List, Dict, Optional
from dataclasses import dataclass, field
from enum import Enum
from difflib import get_close_matches
import argparse
import os


@dataclass
class Config:
    """User configuration loaded from ~/.config/refcheck/config.toml."""
    time_window: str = "6 months"
    stale_threshold_days: int = 7
    show_no_rules_hint: bool = True


def parse_duration_to_days(duration: str) -> int:
    """
    Parse a human-readable duration string to days.

    Examples: "1 week" -> 7, "2 weeks" -> 14, "1 month" -> 30, "6 months" -> 180
    """
    duration = duration.strip().lower()

    match = re.match(r'(\d+)\s*(day|days|week|weeks|month|months)', duration)
    if not match:
        return 7

    value = int(match.group(1))
    unit = match.group(2)

    if unit.startswith('day'):
        return value
    elif unit.startswith('week'):
        return value * 7
    elif unit.startswith('month'):
        return value * 30

    return 7


def load_config() -> Config:
    """
    Load user configuration from ~/.config/refcheck/config.toml.

    Returns default config if file doesn't exist or can't be parsed.
    """
    config_path = Path.home() / ".config" / "refcheck" / "config.toml"
    config = Config()

    if not config_path.exists():
        return config

    try:
        import tomllib
    except ImportError:
        try:
            import tomli as tomllib
        except ImportError:
            return config

    try:
        with open(config_path, "rb") as f:
            data = tomllib.load(f)

        learn = data.get("learn", {})
        if "time_window" in learn:
            config.time_window = learn["time_window"]

        warnings = data.get("warnings", {})
        if "stale_threshold" in warnings:
            config.stale_threshold_days = parse_duration_to_days(warnings["stale_threshold"])
        if "show_no_rules_hint" in warnings:
            config.show_no_rules_hint = warnings["show_no_rules_hint"]

    except Exception:
        pass

    return config


class CheckType(Enum):
    PATTERN = "old_path_pattern"
    SOURCE = "broken_source_command"
    SCRIPT = "broken_bash_command"
    FRAGILE_CWD = "fragile_cwd_path"
    FRAGILE_REFACTOR = "fragile_traversal_path"


@dataclass
class Issue:
    file: Path
    line_num: int
    check_type: CheckType
    message: str
    suggestion: Optional[str] = None
    similar_files: List[str] = field(default_factory=list)


@dataclass
class Warning:
    file: Path
    line_num: int
    check_type: CheckType
    message: str
    suggestion: Optional[str] = None


class ReferenceChecker:
    """Validates file references across a codebase."""

    DEFAULT_EXCLUDES = {
        ".git",
        "node_modules",
        ".venv",
        "site",
        "__pycache__",
        ".cache",
    }

    DEFAULT_EXCLUDE_PATTERNS = [
        "*.pyc",
        ".claude/metrics/**",
        ".planning/**",
        "site/**",
    ]

    # Patterns excluded in normal mode but included with --test-mode
    TEST_FIXTURE_PATTERNS = [
        "tests/apps/test-refcheck.sh",
        "tests/apps/fixtures/refcheck-*/**",
        "docs/archive/**",
    ]

    DYNAMIC_PATH_PATTERNS = [
        r"^\$",
        r"^/tmp/",
        r"^/root/",
        r"^/home/",
        r"^/Users/",
        r"/nvm\.sh$",
        r"^/lib/lib\.sh",
    ]

    def __init__(
        self,
        root_dir: Path = None,
        search_path: Path = None,
        skip_docs: bool = False,
        file_type: str = None,
        warn_fragile: bool = True,
        strict: bool = False,
        test_mode: bool = False,
        config: Config = None,
    ):
        self.root_dir = root_dir or Path.cwd()
        self.search_path = search_path or self.root_dir
        self.skip_docs = skip_docs
        self.file_type = file_type
        self.warn_fragile = warn_fragile
        self.strict = strict
        self.test_mode = test_mode
        self.config = config or Config()
        self.issues: List[Issue] = []
        self.warnings: List[Warning] = []
        self.exclude_dirs = self.DEFAULT_EXCLUDES.copy()
        self.exclude_patterns = self.DEFAULT_EXCLUDE_PATTERNS.copy()
        self._file_index: Optional[List[Path]] = None
        self._rules: Optional[Dict] = None

        # Exclude test fixtures unless in test mode
        if not test_mode:
            self.exclude_patterns.extend(self.TEST_FIXTURE_PATTERNS)

    def should_skip_file(self, file_path: Path) -> bool:
        """Determine if file should be skipped."""
        for part in file_path.parts:
            if part in self.exclude_dirs:
                return True

        for pattern in self.exclude_patterns:
            if file_path.match(pattern):
                return True

        if file_path.suffix in {".pyc", ".so", ".o", ".a", ".dylib"}:
            return True

        return False

    def is_dynamic_path(self, path: str) -> bool:
        """Check if path is dynamic/runtime-generated."""
        for pattern in self.DYNAMIC_PATH_PATTERNS:
            if re.search(pattern, path):
                return True
        return False

    def build_file_index(self) -> List[Path]:
        """Build index of all files in repo for suggestion matching."""
        if self._file_index is not None:
            return self._file_index

        self._file_index = []
        for file_path in self.root_dir.rglob("*"):
            if not file_path.is_file():
                continue
            try:
                rel_path = file_path.relative_to(self.root_dir)
            except ValueError:
                continue
            if self.should_skip_file(rel_path):
                continue
            self._file_index.append(rel_path)
        return self._file_index

    def get_repo_root(self) -> Optional[Path]:
        """Get git repo root, or None if not in a repo."""
        try:
            result = subprocess.run(
                ["git", "rev-parse", "--show-toplevel"],
                capture_output=True,
                text=True,
                check=True,
                cwd=self.root_dir,
            )
            return Path(result.stdout.strip())
        except (subprocess.CalledProcessError, FileNotFoundError):
            return None

    def get_rules_path(self, repo_root: Path) -> Path:
        """Get the rules file path for a given repo root."""
        # Make safe directory name: /Users/chris/dotfiles -> Users--chris--dotfiles
        safe_name = str(repo_root).lstrip("/").replace("/", "--")
        return Path.home() / ".config" / "refcheck" / "repos" / safe_name / "rules.json"

    def load_rules(self) -> Dict:
        """
        Load rules from ~/.config/refcheck/repos/{safe-repo-path}/rules.json.

        Rules are per-repo based on the git repository root.
        """
        if self._rules is not None:
            return self._rules

        self._rules = {"directory_mappings": {}, "file_mappings": {}}
        self._rules_path = None  # Track path for hint message

        repo_root = self.get_repo_root()
        if repo_root is None:
            return self._rules

        rules_path = self.get_rules_path(repo_root)
        self._rules_path = rules_path

        if rules_path.exists():
            try:
                import json
                with open(rules_path) as f:
                    loaded = json.load(f)
                    if loaded:
                        self._rules = loaded
            except Exception:
                pass

        return self._rules

    def get_rules_age_days(self) -> Optional[int]:
        """
        Get the age of the rules file in days.

        Returns None if rules don't exist or have no timestamp.
        """
        rules = self.load_rules()
        metadata = rules.get("_metadata", {})
        generated = metadata.get("generated")

        if not generated:
            return None

        try:
            from datetime import datetime
            generated_dt = datetime.fromisoformat(generated)
            now = datetime.now()
            delta = now - generated_dt
            return delta.days
        except (ValueError, TypeError):
            return None

    def find_similar_files(self, missing_path: str) -> List[str]:
        """
        Find files that might be renamed versions of missing_path.

        Searches for:
        1. Exact basename matches anywhere in repo
        2. Transform variants (hyphen â†” underscore, case changes)
        3. Fuzzy matches using difflib
        4. Known mappings from rules file
        """
        file_index = self.build_file_index()
        rules = self.load_rules()
        basename = Path(missing_path).name
        parent_dir = str(Path(missing_path).parent)
        suggestions = []
        seen_paths = set()

        def add_suggestion(path: str, reason: str):
            if path not in seen_paths:
                seen_paths.add(path)
                suggestions.append(f"{path} ({reason})")

        # Check directory mappings from rules
        for old_prefix, new_prefix in rules.get("directory_mappings", {}).items():
            if missing_path.startswith(old_prefix):
                mapped_path = missing_path.replace(old_prefix, new_prefix, 1)
                if (self.root_dir / mapped_path).exists():
                    add_suggestion(mapped_path, "known mapping")

        # Check file mappings from rules
        if basename in rules.get("file_mappings", {}):
            new_name = rules["file_mappings"][basename]
            for f in file_index:
                if f.name == new_name:
                    add_suggestion(str(f), "known mapping")

        # 1. Exact basename match
        for f in file_index:
            if f.name == basename:
                add_suggestion(str(f), "basename match")

        # 2. Transform variants (hyphen â†” underscore)
        variants = {
            basename.replace("-", "_"),
            basename.replace("_", "-"),
            basename.lower(),
            basename.replace("-", "_").lower(),
            basename.replace("_", "-").lower(),
        }
        variants.discard(basename)

        for f in file_index:
            if f.name in variants:
                add_suggestion(str(f), "name variant")

        # 3. Fuzzy match using difflib (only if few suggestions so far)
        if len(suggestions) < 3:
            all_names = [f.name for f in file_index]
            matches = get_close_matches(basename, all_names, n=3, cutoff=0.8)
            for m in matches:
                if m != basename and m not in variants:
                    for f in file_index:
                        if f.name == m:
                            add_suggestion(str(f), "similar name")
                            break

        return suggestions[:5]

    def find_files(self, pattern: str = "**/*") -> List[Path]:
        """Find all files matching pattern, respecting exclusions."""
        files = []
        search_root = self.search_path

        for file_path in search_root.glob(pattern):
            if not file_path.is_file():
                continue

            try:
                rel_path = file_path.relative_to(self.root_dir)
            except ValueError:
                continue

            if self.should_skip_file(rel_path):
                continue

            if self.file_type and file_path.suffix != f".{self.file_type}":
                continue

            files.append(file_path)

        return files

    def check_pattern(self, pattern: str, description: str = None):
        """Check for old path pattern references."""
        description = description or f"Old pattern: {pattern}"

        for file_path in self.find_files():
            if self.skip_docs and file_path.suffix == ".md":
                continue

            # Skip verification scripts themselves
            if file_path.name in ("refcheck", "verify-references.py", "verify-file-references.sh"):
                continue

            try:
                rel_path = file_path.relative_to(self.root_dir)
                with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                    for line_num, line in enumerate(f, 1):
                        if pattern in line:
                            self.issues.append(
                                Issue(
                                    file=rel_path,
                                    line_num=line_num,
                                    check_type=CheckType.PATTERN,
                                    message=f"Found: {pattern}",
                                    suggestion=description,
                                )
                            )
            except (OSError, UnicodeDecodeError):
                continue

    def check_source_statements(self):
        """Check that source statements point to existing files."""
        source_pattern = re.compile(
            r'source\s+["\']([^"\']+)["\']|source\s+\$[^/]*(/[^\s]+)'
        )

        for file_path in self.find_files("**/*.sh"):
            try:
                rel_path = file_path.relative_to(self.root_dir)

                # Build symbol table for this file
                symbol_table = self.parse_variable_assignments(file_path)

                with open(file_path, "r", encoding="utf-8") as f:
                    for line_num, line in enumerate(f, 1):
                        if "source" not in line:
                            continue

                        match = source_pattern.search(line)
                        if not match:
                            continue

                        source_path = match.group(1) or match.group(2)
                        if not source_path:
                            continue

                        # Check if path is dynamic BEFORE trying to resolve variables
                        # (only check original path, not resolved paths)
                        if not '$' in source_path and self.is_dynamic_path(source_path):
                            continue

                        # Try to resolve variables in the path
                        original_path = source_path
                        if '$' in source_path:
                            try:
                                source_path = self.resolve_path(source_path, symbol_table, file_path)
                            except ValueError:
                                # Cannot resolve this path - skip gracefully
                                continue

                        if source_path.startswith("/"):
                            resolved = Path(source_path)
                        else:
                            resolved = self.root_dir / source_path

                        if not resolved.exists():
                            similar = self.find_similar_files(source_path)
                            self.issues.append(
                                Issue(
                                    file=rel_path,
                                    line_num=line_num,
                                    check_type=CheckType.SOURCE,
                                    message=f"Missing: {original_path}" + (
                                        f" â†’ {source_path}" if original_path != source_path else ""
                                    ),
                                    suggestion="Verify path exists or update reference",
                                    similar_files=similar,
                                )
                            )
            except (OSError, UnicodeDecodeError):
                continue

    def go_up_n_levels(self, file_path: Path, n: int) -> Path:
        """Go up N directory levels from file_path."""
        path = file_path.parent
        for _ in range(n - 1):
            path = path.parent
        return path

    def find_repo_root(self, file_path: Path) -> Path:
        """Find git repo root from file path."""
        current = file_path.parent
        while current != current.parent:  # Stop at filesystem root
            if (current / ".git").exists():
                return current
            current = current.parent
        # If no .git found, return current root_dir
        return self.root_dir

    def parse_variable_assignments(self, file_path: Path) -> Dict[str, str]:
        """
        Parse common shell variable assignment patterns.
        Returns dict of {var_name: computed_value}
        """
        symbol_table = {}

        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
        except (OSError, UnicodeDecodeError):
            return symbol_table

        # Pattern 1: SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
        if re.search(r'SCRIPT_DIR="\$\(cd.*BASH_SOURCE', content):
            symbol_table['SCRIPT_DIR'] = str(file_path.parent)

        # Pattern 2: DOTFILES_DIR="$(cd "$SCRIPT_DIR/../.." && pwd)"
        # Extract the number of ../ levels
        match = re.search(r'DOTFILES_DIR="\$\(cd "\$SCRIPT_DIR/((?:\.\./?)+)" && pwd\)"', content)
        if match:
            relative_path = match.group(1)
            levels_up = relative_path.count('..')
            # We need to go up from the file's parent directory
            symbol_table['DOTFILES_DIR'] = str(self.go_up_n_levels(file_path, levels_up + 1))

        # Pattern 3: DOTFILES_DIR="${DOTFILES_DIR:-$HOME/dotfiles}"
        if re.search(r'DOTFILES_DIR="\$\{DOTFILES_DIR:-\$HOME/dotfiles\}"', content):
            # Use actual repo root if we can detect it
            symbol_table['DOTFILES_DIR'] = str(self.find_repo_root(file_path))

        return symbol_table

    def resolve_path(self, path_str: str, symbol_table: Dict[str, str], file_path: Path) -> str:
        """
        Resolve a path containing shell variables.

        Args:
            path_str: Path that may contain variables (e.g., "$SCRIPT_DIR/helpers.sh")
            symbol_table: Dictionary of variable names to their resolved values
            file_path: The file being checked (for context)

        Returns:
            Resolved path with variables substituted

        Raises:
            ValueError: If path contains unresolvable variables
        """
        resolved = path_str

        # Substitute known variables from symbol table
        for var_name, var_value in symbol_table.items():
            # Handle both $VAR and ${VAR} syntax
            resolved = resolved.replace(f"${var_name}", var_value)
            resolved = resolved.replace(f"${{{var_name}}}", var_value)

        # Check if there are still unresolved variables
        if '$' in resolved:
            # Can't resolve this path - raise exception
            raise ValueError(f"Cannot resolve variables in: {path_str}")

        return resolved

    def check_script_references(self):
        """Check that bash script references point to existing files."""
        script_pattern = re.compile(r'(?:bash|sh)\s+["\']?([^\s"\']+\.sh)["\']?')

        for file_path in self.find_files("**/*.sh"):
            if self.skip_docs and file_path.suffix == ".md":
                continue

            try:
                rel_path = file_path.relative_to(self.root_dir)
                with open(file_path, "r", encoding="utf-8") as f:
                    for line_num, line in enumerate(f, 1):
                        if "bash" not in line and "sh" not in line:
                            continue

                        for match in script_pattern.finditer(line):
                            script_path = match.group(1).rstrip('"\'')

                            if not script_path or self.is_dynamic_path(script_path):
                                continue

                            # Skip self-references in usage comments
                            if (
                                line.strip().startswith("#")
                                and script_path == file_path.name
                            ):
                                continue

                            if script_path.startswith("/"):
                                resolved = Path(script_path)
                            else:
                                resolved = self.root_dir / script_path

                            if not resolved.exists():
                                similar = self.find_similar_files(script_path)
                                self.issues.append(
                                    Issue(
                                        file=rel_path,
                                        line_num=line_num,
                                        check_type=CheckType.SCRIPT,
                                        message=f"Missing: {script_path}",
                                        suggestion="Verify script exists or update reference",
                                        similar_files=similar,
                                    )
                                )
            except (OSError, UnicodeDecodeError):
                continue

    def check_relative_path_fragility(self):
        """Check if relative paths are fragile to working directory changes."""
        # Match: source "path" OR source 'path' OR source path (unquoted)
        source_pattern = re.compile(
            r'source\s+(?:["\']([^"\']+)["\']|([^\s]+))'
        )

        for file_path in self.find_files("**/*.sh"):
            try:
                rel_path = file_path.relative_to(self.root_dir)

                # Build symbol table for this file
                symbol_table = self.parse_variable_assignments(file_path)

                with open(file_path, "r", encoding="utf-8") as f:
                    for line_num, line in enumerate(f, 1):
                        if "source" not in line:
                            continue

                        match = source_pattern.search(line)
                        if not match:
                            continue

                        source_path = match.group(1) or match.group(2)
                        if not source_path:
                            continue

                        # Skip if it has variables (will be resolved) or is absolute or dynamic
                        if '$' in source_path or source_path.startswith("/") or self.is_dynamic_path(source_path):
                            continue

                        # This is a plain relative path - test from multiple directories
                        test_dirs = [
                            self.root_dir,              # Repo root
                            file_path.parent,           # Script's directory
                            file_path.parent.parent,    # One level up
                        ]

                        valid_from = []
                        for test_dir in test_dirs:
                            resolved = test_dir / source_path
                            if resolved.exists():
                                valid_from.append(test_dir)

                        # If it only works from some directories (not all), it's fragile
                        if len(valid_from) > 0 and len(valid_from) < len(test_dirs):
                            try:
                                valid_desc = ", ".join(str(d.relative_to(self.root_dir)) if d != self.root_dir else "repo root" for d in valid_from)
                            except ValueError:
                                valid_desc = ", ".join(str(d) for d in valid_from)

                            self.warnings.append(
                                Warning(
                                    file=rel_path,
                                    line_num=line_num,
                                    check_type=CheckType.FRAGILE_CWD,
                                    message=f"Relative path only valid from: {valid_desc}",
                                    suggestion="Use absolute path or root directory variable (e.g., $PROJECT_ROOT, $REPO_ROOT)",
                                )
                            )
            except (OSError, UnicodeDecodeError):
                continue

    def check_relative_traversal(self):
        """Detect relative directory traversal patterns fragile to file moves."""
        # Pattern: VARIABLE_DIR="$(cd "$OTHER_DIR/../../.." && pwd)"
        traversal_pattern = re.compile(
            r'([A-Z_]+_DIR)=.*\$\(cd.*\.\./.*pwd\)'
        )

        for file_path in self.find_files("**/*.sh"):
            try:
                rel_path = file_path.relative_to(self.root_dir)

                with open(file_path, "r", encoding="utf-8") as f:
                    for line_num, line in enumerate(f, 1):
                        match = traversal_pattern.search(line)
                        if not match:
                            continue

                        var_name = match.group(1)

                        self.warnings.append(
                            Warning(
                                file=rel_path,
                                line_num=line_num,
                                check_type=CheckType.FRAGILE_REFACTOR,
                                message=f"{var_name} uses relative directory traversal (../) - fragile to file moves",
                                suggestion="Consider dynamic root detection: git rev-parse --show-toplevel, or search for project marker (.git, etc.)",
                            )
                        )
            except (OSError, UnicodeDecodeError):
                continue

    def run_all_checks(self):
        """Run all validation checks."""
        # Load rules early so we can show hint if missing
        self.load_rules()

        self.check_source_statements()
        self.check_script_references()

        # Only run warning checks if enabled
        if self.warn_fragile:
            self.check_relative_path_fragility()
            self.check_relative_traversal()

    def print_results(self):
        """Print validation results."""
        try:
            search_info = f" in {self.search_path.relative_to(self.root_dir)}" if self.search_path != self.root_dir else ""

            # Show hint if no rules file exists for this repo
            if hasattr(self, '_rules_path') and self._rules_path and not self._rules_path.exists():
                if self.config.show_no_rules_hint:
                    print("\nðŸ’¡ No learned rules found. Run 'refcheck --learn-rules' to learn from git rename history.\n")
            else:
                # Check for stale rules (older than threshold)
                age_days = self.get_rules_age_days()
                if age_days is not None and age_days > self.config.stale_threshold_days:
                    print(f"\nâš ï¸  Rules last updated {age_days} days ago. Run 'refcheck --learn-rules' to refresh.\n")

            # If no issues or warnings, success!
            if not self.issues and not self.warnings:
                print(f"\nâœ… All file references valid{search_info}\n")
                return

            # Print summary header
            error_count = len(self.issues)
            warning_count = len(self.warnings)

            if error_count > 0 and warning_count > 0:
                print(f"\nâŒ Found {error_count} error(s) and {warning_count} warning(s){search_info}\n")
            elif error_count > 0:
                print(f"\nâŒ Found {error_count} error(s){search_info}\n")
            else:
                print(f"\nâš ï¸  Found {warning_count} warning(s){search_info}\n")

            # Print errors
            if self.issues:
                print("Errors:")
                print("â”€" * 60)

                by_type: Dict[CheckType, List[Issue]] = {}
                for issue in self.issues:
                    by_type.setdefault(issue.check_type, []).append(issue)

                for check_type, issues in sorted(by_type.items(), key=lambda x: x[0].value):
                    print(f"\n{check_type.value.replace('_', ' ').title()} ({len(issues)}):")
                    print("â”€" * 60)
                    for issue in issues:
                        print(f"  {issue.file}:{issue.line_num}")
                        print(f"    {issue.message}")
                        if issue.similar_files:
                            print("    â†’ Possible matches:")
                            for similar in issue.similar_files:
                                print(f"        {similar}")
                        if issue.suggestion:
                            print(f"    â†’ {issue.suggestion}")

            # Print warnings
            if self.warnings:
                if self.issues:
                    print()  # Extra space between errors and warnings

                print("Warnings:")
                print("â”€" * 60)

                by_type: Dict[CheckType, List[Warning]] = {}
                for warning in self.warnings:
                    by_type.setdefault(warning.check_type, []).append(warning)

                for check_type, warnings in sorted(by_type.items(), key=lambda x: x[0].value):
                    print(f"\n{check_type.value.replace('_', ' ').title()} ({len(warnings)}):")
                    print("â”€" * 60)
                    for warning in warnings:
                        print(f"  {warning.file}:{warning.line_num}")
                        print(f"    {warning.message}")
                        if warning.suggestion:
                            print(f"    â†’ {warning.suggestion}")

            print()
        except BrokenPipeError:
            sys.stderr.close()
            pass


def learn_rules_from_git(time_window: str = "6 months") -> None:
    """
    Scan git history for renames and generate rules.json.

    Always operates from the git repo root, regardless of cwd.
    Analyzes git rename operations from the specified time window to extract:
    - Directory prefix mappings (e.g., management/tests/ â†’ tests/install/)
    - File name mappings (e.g., parse-packages.py â†’ parse_packages.py)

    Args:
        time_window: Git time specification (e.g., "6 months", "1 year", "3 weeks")
    """
    import json
    from collections import defaultdict
    from datetime import datetime

    # Find repo root first
    try:
        root_result = subprocess.run(
            ["git", "rev-parse", "--show-toplevel"],
            capture_output=True,
            text=True,
            check=True,
        )
        repo_root = Path(root_result.stdout.strip())
    except subprocess.CalledProcessError:
        print("fatal: not a git repository (or any of the parent directories): .git", file=sys.stderr)
        sys.exit(128)

    # Run git log to get renames from repo root using time-based window
    try:
        result = subprocess.run(
            ["git", "log", f"--since={time_window} ago", "--diff-filter=R", "-M", "--name-status", "--format=%H %aI"],
            capture_output=True,
            text=True,
            check=True,
            cwd=repo_root,
        )
    except subprocess.CalledProcessError as e:
        print(f"Error running git: {e}", file=sys.stderr)
        sys.exit(1)

    # Parse renames
    directory_mappings = defaultdict(int)
    file_mappings = {}
    commits_analyzed = set()

    for line in result.stdout.splitlines():
        line = line.strip()
        if not line:
            continue

        # Check if this is a commit line (hash + date)
        if " " in line and len(line.split()[0]) == 40:
            commits_analyzed.add(line.split()[0][:8])
            continue

        # Check if this is a rename line
        if line.startswith("R"):
            parts = line.split("\t")
            if len(parts) == 3:
                old_path = parts[1]
                new_path = parts[2]

                old_name = Path(old_path).name
                new_name = Path(new_path).name
                old_dir = str(Path(old_path).parent)
                new_dir = str(Path(new_path).parent)

                # Track file name changes
                if old_name != new_name:
                    file_mappings[old_name] = new_name

                # Track directory changes with frequency
                if old_dir != new_dir and old_dir != ".":
                    old_parts = old_dir.split("/")
                    new_parts = new_dir.split("/")

                    # Only track if directories differ meaningfully
                    if old_parts[0] != new_parts[0] or len(old_parts) != len(new_parts):
                        old_key = f"{old_dir}/"
                        new_val = f"{new_dir}/"
                        # Track frequency: store (new_dir, count)
                        if old_key in directory_mappings:
                            existing_new, count = directory_mappings[old_key]
                            directory_mappings[old_key] = (existing_new, count + 1)
                        else:
                            directory_mappings[old_key] = (new_val, 1)

    # Create rules directory based on repo root
    safe_name = str(repo_root).lstrip("/").replace("/", "--")
    rules_dir = Path.home() / ".config" / "refcheck" / "repos" / safe_name
    rules_dir.mkdir(parents=True, exist_ok=True)
    rules_path = rules_dir / "rules.json"

    # Build rules dict - take top 20 directory mappings by frequency
    sorted_dirs = sorted(directory_mappings.items(), key=lambda x: -x[1][1])
    rules = {
        "_metadata": {
            "generated": datetime.now().isoformat()[:19],
            "time_window": time_window,
            "commits_analyzed": len(commits_analyzed),
        },
        "directory_mappings": {k: v[0] for k, v in sorted_dirs[:20]},
        "file_mappings": file_mappings,
    }

    # Write to file
    with open(rules_path, "w") as f:
        json.dump(rules, f, indent=2)

    print(f"âœ… Generated {rules_path}")
    print(f"   Time window: {time_window}")
    print(f"   Commits analyzed: {len(commits_analyzed)}")
    print(f"   Directory mappings: {len(rules['directory_mappings'])}")
    print(f"   File mappings: {len(file_mappings)}")


def main():
    parser = argparse.ArgumentParser(
        prog="refcheck",
        description="Find broken file references and old path patterns",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Validate all references + warn about fragile paths
  %(prog)s
  %(prog)s --skip-docs              # Skip markdown files

  # Check specific directory
  %(prog)s management/
  %(prog)s apps/ --type sh          # Only shell scripts in apps/

  # Control warnings
  %(prog)s --no-warn                # Only check errors, skip fragile path warnings
  %(prog)s --strict                 # Treat warnings as errors (CI mode)

  # Find old patterns after refactoring
  %(prog)s --pattern "old/path/"
  %(prog)s --pattern "FooClass" --desc "Renamed to BarClass"

What it checks:
  Errors (always checked, exit 1):
    - Broken source commands (source statements pointing to non-existent files)
    - Broken bash commands (bash/sh invocations pointing to non-existent scripts)
    - Old path patterns (after moving/renaming directories)

  Warnings (checked by default, exit 0 unless --strict):
    - Fragile CWD paths: Relative paths that only work from specific directories
    - Fragile traversal paths: Directory variables using ../ traversal (fragile to file moves)

Learn from git history:
  %(prog)s --learn-rules           # Generate ~/.config/refcheck/rules.yml from git renames

Exit codes:
  0 - No errors (warnings OK unless --strict)
  1 - Found errors, or warnings in --strict mode
        """,
    )
    parser.add_argument(
        "path",
        nargs="?",
        type=Path,
        help="Directory to check (default: current directory)",
    )
    parser.add_argument(
        "--pattern",
        metavar="PATTERN",
        help="Check for specific old path pattern (e.g., 'old/path/')",
    )
    parser.add_argument(
        "--desc",
        metavar="DESC",
        help="Description for pattern check",
    )
    parser.add_argument(
        "--type",
        "-t",
        metavar="TYPE",
        help="Filter by file type (e.g., 'sh', 'py')",
    )
    parser.add_argument(
        "--skip-docs",
        action="store_true",
        help="Skip documentation (.md) files",
    )
    parser.add_argument(
        "--strict",
        action="store_true",
        help="Treat warnings as errors (exit 1 if warnings found)",
    )
    parser.add_argument(
        "--no-warn",
        action="store_true",
        help="Disable fragile path warnings (only check for errors)",
    )
    parser.add_argument(
        "--test-mode",
        action="store_true",
        help="Include test fixtures (normally excluded)",
    )
    parser.add_argument(
        "--learn-rules",
        action="store_true",
        help="Generate rules.json from git rename history",
    )
    args = parser.parse_args()

    # Load user configuration
    config = load_config()

    # Handle --learn-rules before other operations
    if args.learn_rules:
        learn_rules_from_git(config.time_window)
        sys.exit(0)

    root_dir = Path.cwd()
    search_path = args.path.resolve() if args.path else root_dir

    # Ensure search_path is within or is root_dir
    try:
        search_path.relative_to(root_dir)
    except ValueError:
        # search_path is outside root_dir, use it as new root
        root_dir = search_path

    checker = ReferenceChecker(
        root_dir=root_dir,
        search_path=search_path,
        skip_docs=args.skip_docs,
        file_type=args.type,
        warn_fragile=not args.no_warn,
        strict=args.strict,
        test_mode=args.test_mode,
        config=config,
    )

    if args.pattern:
        checker.check_pattern(args.pattern, args.desc)
    else:
        checker.run_all_checks()

    checker.print_results()

    # Exit code logic:
    # - Always exit 1 if there are errors
    # - Exit 1 if strict mode and there are warnings
    # - Otherwise exit 0
    if checker.issues:
        sys.exit(1)
    elif checker.strict and checker.warnings:
        sys.exit(1)
    else:
        sys.exit(0)


if __name__ == "__main__":
    main()
