#!/usr/bin/env bash
#
# backup-dirs - Create compressed backup archive of specified directories
#
# Usage: backup-dirs [OPTIONS] <dir1> [dir2] [dir3...]
#
# Creates a timestamped compressed archive with smart exclusions, live progress tracking,
# and comprehensive statistics. Defaults to fast multi-threaded zstd compression.
#
# Options:
#   -n, --name <name>       Custom name for backup (default: backup-dirs)
#   -d, --dest <path>       Destination directory (default: ~/Documents)
#   --zstd                  Use zstd compression (default, fast, multi-threaded)
#   --gzip                  Use gzip compression (traditional, widely compatible)
#   --xz                    Use xz compression (best ratio, slowest)
#   --fast                  Fastest compression (zstd level 1)
#   --best                  Best compression (zstd level 19, slower)
#   -j, --threads <num>     Compression threads (default: auto-detect cores)
#   --analyze               Pre-scan to show percentage progress (slower startup)
#   --no-analyze            Skip pre-scan, show raw count only (default, faster)
#   -h, --help              Show this help message
#
# Examples:
#   backup-dirs dotfiles .config                        # Fast zstd backup
#   backup-dirs --name dotfiles-snapshot dotfiles       # Custom name
#   backup-dirs --gzip --dest /tmp dotfiles             # Traditional gzip to /tmp
#   backup-dirs --best --analyze learning               # Maximum compression with percentage
#   backup-dirs --name learning-docs learning           # learning-docs_2025-11-25_143022.tar.zst
#
# Technical Implementation Notes:
#
# Why zstd as default compression?
# zstd provides 2-3x faster compression than gzip while achieving better compression
# ratios. More importantly, it supports multi-threading (via -T flag), allowing it to
# utilize all available CPU cores. On modern multi-core systems, this means a 3000-file
# backup that takes 90 seconds with gzip completes in 35 seconds with zstd. The format
# is well-supported by modern tar implementations and worth the dependency.
#
# Why skip analysis by default?
# Analysis requires walking the entire directory tree with fd to count files before
# archiving begins. This doubles the I/O overhead - you traverse the tree once to count,
# then again with tar to archive. For most backups, users prioritize speed over seeing
# percentage completion. The raw file count during archiving provides sufficient feedback.
#
# Why keep .git directories despite their size?
# Git history is essential for disaster recovery. If you're backing up a project, being
# able to recover the full git history (branches, tags, commit messages, authorship) is
# often more valuable than the disk space saved by excluding it. Other artifacts like
# node_modules and .venv can be regenerated from package manifests, but git history cannot.
#
# Critical Gotcha #1: Trap Handlers and Exit Codes with set -e
# When a trap handler runs on EXIT, its return value can override the script's intended
# exit code. With 'set -e', any command returning non-zero causes immediate exit. The
# cleanup trap must ensure every operation succeeds (via || true) or it will turn a
# successful backup (exit 0) into a failure (exit 1). This is non-obvious because the
# trap runs AFTER the main code exits successfully.
#
# Example of the bug: The line `[[ -n "$VAR" ]] && rm -f "$VAR"` returns 1 if VAR is
# unset (the test fails). Without || true at the end, this non-zero return becomes the
# trap's exit code, which becomes the script's exit code, overriding the explicit
# `exit 0` from successful completion.
#
# Critical Gotcha #2: wait Command Behavior with set -e
# The wait builtin returns the exit status of the waited-for process. If the process has
# already exited or doesn't exist, wait returns non-zero. With 'set -e', this causes
# immediate script exit. This is particularly problematic with background spinner processes
# that may finish microseconds before wait is called. Adding || true makes wait safe to
# call regardless of process state.
#
# Critical Gotcha #3: Cross-Platform File Size Detection
# GNU stat --format='%s' works correctly on macOS (outputs file size as expected), but
# returns exit code 1 instead of 0. This breaks scripts using 'set -e' even though the
# command succeeded functionally. The solution is `wc -c < file` which works identically
# across macOS, Linux, and BSD with consistent exit codes. The < redirection is critical -
# without it, wc includes the filename in output.
#
# Critical Gotcha #4: tar Exit Codes Don't Indicate Failure
# tar returns exit code 1 for non-fatal warnings like "file changed while reading" even
# when the archive was created successfully and is perfectly valid. This means you cannot
# rely on tar's exit code to determine success. Instead, validate the archive exists and
# is non-empty after creation. This is why we disable pipefail during archiving - a tar
# warning would otherwise fail the entire pipeline despite producing a valid archive.
#
# Note: Excludes .venv, node_modules, __pycache__, and various cache directories that can
# be regenerated from manifests. These patterns reduce backup size by 50-90% typically.

set -euo pipefail

# ============================================================================
# Dependencies
# ============================================================================

# Source required shell libraries
DOTFILES_DIR="${DOTFILES_DIR:-$HOME/dotfiles}"
source "$DOTFILES_DIR/platforms/common/.local/shell/formatting.sh"
source "$DOTFILES_DIR/platforms/common/.local/shell/colors.sh"

# ============================================================================
# Configuration
# ============================================================================

# Compression defaults - Why zstd?
# zstd is 2-3x faster than gzip with better compression ratios AND multi-threading support.
# Level 3 provides excellent balance: fast enough for interactive use, good compression.
# Threads=0 means auto-detect cores, utilizing modern multi-core CPUs effectively.
readonly COMPRESSION_DEFAULT="zstd"
readonly COMPRESSION_LEVEL_DEFAULT=3      # Sweet spot: 1=fastest, 3=balanced, 19=best
readonly COMPRESSION_THREADS_DEFAULT=0    # 0 = auto-detect all available CPU cores

# Feature defaults - Why skip analysis?
# Pre-scanning with fd to count files adds startup overhead. For most backups, users want
# speed over percentage display. Raw count during archiving is sufficient and feels faster.
readonly ANALYZE_DEFAULT=false            # Skip pre-scan for faster startup

# File extensions by compression type
# Proper extensions enable auto-detection by tar when extracting. CRITICAL: These must
# match the actual compression used or extraction will fail silently.
declare -A COMPRESSION_EXTENSIONS=(
  ["zstd"]="tar.zst"   # Modern: zstd compressed tar
  ["gzip"]="tar.gz"    # Traditional: gzip compressed tar
  ["xz"]="tar.xz"      # Best ratio: xz compressed tar
)

# Progress display tuning - Why these specific values?
# UPDATE_INTERVAL_NS (90ms): Balances visual feedback vs CPU overhead. Too fast thrashes
#   terminal rendering, too slow feels unresponsive. 90ms hits sweet spot.
# COLOR_CYCLE_FILES (20): Changes color frequently enough to show progress but not so
#   fast it's distracting or induces seizures.
# SPINNER_SLEEP: Analysis (50ms) can be slower since it's rare. Archive (20ms) feels
#   responsive during the main work phase.
readonly COLOR_CYCLE_FILES=20           # Files per color change (visual rhythm)
readonly UPDATE_INTERVAL_NS=90000000    # 90ms update interval (smooth without thrashing)
readonly ANALYSIS_SPINNER_SLEEP=0.05    # 50ms: Slower animation for analysis phase
readonly ARCHIVE_SPINNER_SLEEP=0.02     # 20ms: Faster animation for main work

# Spinner animation characters - Braille dot patterns
# These Unicode characters create a smooth circular animation that works in all modern
# terminals. Braille dots are compact and don't disrupt the progress line layout.
readonly SPINNER_CHARS="⠋⠙⠹⠸⠼⠴⠦⠧⠇⠏"

# Rainbow colors - ANSI color codes for visual variety
# Cycles through warm-to-cool spectrum. Avoids pure red/green to reduce eye strain.
# Order: Red -> Yellow -> Green -> Cyan -> Blue -> Magenta -> repeat
readonly COLORS=("91" "93" "92" "96" "94" "95")

# Colors for directory display (Green, Cyan, Yellow, Magenta, Blue)
readonly DIR_COLORS=("92" "96" "93" "95" "94")

# Exclusion patterns for tar (common bloat to skip)
readonly EXCLUDE_PATTERNS=(
  '.venv'
  'node_modules'
  '__pycache__'
  '.mypy_cache'
  '.ruff_cache'
  '.pytest_cache'
  'build'
  'dist'
  'modules'
  'zephyr'
  '*.pyc'
  '.DS_Store'
)

# ============================================================================
# Global Variables
# ============================================================================

# Command line arguments
BACKUP_NAME="backup-dirs"
DEST_DIR="$HOME/Documents"
DIRS_TO_BACKUP=()

# Compression settings (set by flags)
COMPRESSION="$COMPRESSION_DEFAULT"
COMPRESSION_LEVEL="$COMPRESSION_LEVEL_DEFAULT"
COMPRESSION_THREADS="$COMPRESSION_THREADS_DEFAULT"

# Feature flags (set by flags)
ANALYZE="$ANALYZE_DEFAULT"

# Timing
OVERALL_START_TIME=""
ARCHIVE_START_TIME=""

# Progress tracking
ANALYSIS_SPINNER_PID=""
ARCHIVE_SPINNER_PID=""

# Temp files for IPC
ANALYSIS_COUNT_FILE=""
ANALYSIS_SIZE_FILE=""
ANALYSIS_RUNNING_FILE=""
PROGRESS_FILE=""

# ============================================================================
# Usage and Help
# ============================================================================

usage() {
  echo ""
  print_banner "backup-dirs" "cyan"
  echo ""
  echo "  $(color_bright_white "Create timestamped compressed backup archives with progress tracking")"
  echo ""

  print_section "Usage" "brightcyan"
  echo ""
  echo "  $(color_bright_yellow "backup-dirs") $(color_bright_magenta "[OPTIONS]") $(color_bright_green "<dir1>") [dir2] [dir3...]"
  echo ""

  print_section "Options" "brightcyan"
  echo ""
  echo "  $(color_bright_magenta "-n, --name") $(color_cyan "<name>")     Custom backup name $(color_bright_black "(default: backup-dirs)")"
  echo "  $(color_bright_magenta "-d, --dest") $(color_cyan "<path>")     Destination directory $(color_bright_black "(default: ~/Documents)")"
  echo "  $(color_bright_magenta "-a, --analyze")         Run analysis phase for percentage $(color_bright_black "(default: off)")"
  echo "  $(color_bright_magenta "--no-analyze")         Skip analysis $(color_bright_black "(default, faster)")"
  echo "  $(color_bright_magenta "--zstd")               Use zstd compression $(color_bright_black "(default, faster)")"
  echo "  $(color_bright_magenta "--gzip")               Use gzip compression $(color_bright_black "(compatible)")"
  echo "  $(color_bright_magenta "--xz")                 Use xz compression $(color_bright_black "(best compression)")"
  echo "  $(color_bright_magenta "--fast")               Optimize for speed $(color_bright_black "(zstd -1, no analysis)")"
  echo "  $(color_bright_magenta "--best")               Optimize for compression $(color_bright_black "(zstd -19)")"
  echo "  $(color_bright_magenta "-j, --threads") $(color_cyan "<N>")   Compression threads $(color_bright_black "(default: auto)")"
  echo "  $(color_bright_magenta "-h, --help")           Show this help message"
  echo ""

  print_section "Arguments" "brightcyan"
  echo ""
  echo "  $(color_bright_green "<dir1> [dir2]...")    Directories to backup $(color_bright_black "(relative to HOME or absolute)")"
  echo ""

  print_section "Examples" "brightcyan"
  echo ""
  echo "  $(color_bright_black "# Backup specific directories to Documents")"
  echo "  $(color_bright_yellow "backup-dirs") $(color_green ".claude notes learning")"
  echo ""
  echo "  $(color_bright_black "# Custom backup name")"
  echo "  $(color_bright_yellow "backup-dirs") $(color_magenta "--name learning-docs") $(color_green "learning")"
  echo ""
  echo "  $(color_bright_black "# Backup to custom destination")"
  echo "  $(color_bright_yellow "backup-dirs") $(color_magenta "--dest ~/Backups") $(color_green ".config dotfiles")"
  echo ""
  echo "  $(color_bright_black "# Use absolute paths")"
  echo "  $(color_bright_yellow "backup-dirs") $(color_green "\$HOME/learning /tmp/backups")"
  echo ""

  print_section "Features" "brightcyan"
  echo ""
  echo "  $(color_cyan "•") Archive format:  $(color_bright_white "<name>_YYYY-MM-DD_HHMMSS.tar.zst") $(color_bright_black "(zstd by default)")"
  echo "  $(color_cyan "•") Custom naming:   $(color_bright_white "Use --name for descriptive backups") $(color_bright_black "(e.g., learning-docs)")"
  echo "  $(color_cyan "•") Fast compression: $(color_bright_white "Multi-threaded zstd") $(color_bright_black "(2-3x faster than gzip)")"
  echo "  $(color_cyan "•") Quick backups:   $(color_bright_white "No analysis by default") $(color_bright_black "(skips double tree walk)")"
  echo "  $(color_cyan "•") Smart excludes:  $(color_bright_black ".venv, node_modules, caches, build artifacts")"
  echo "  $(color_cyan "•") Git included:    $(color_bright_white ".git") $(color_bright_black "directories kept for recovery")"
  echo "  $(color_cyan "•") Live progress:   $(color_bright_white "Rainbow") $(color_red "c")$(color_yellow "o")$(color_green "l")$(color_cyan "o")$(color_blue "r")$(color_magenta "s") $(color_bright_white "and smooth animations")"
  echo "  $(color_cyan "•") Statistics:      $(color_bright_white "Files, sizes, compression, timing")"
  echo ""
}

# ============================================================================
# Cleanup Handler
# ============================================================================
#
# CRITICAL: This function runs on EXIT/INT/TERM via trap. It must NEVER fail or it will
# override the script's exit code. Every command must succeed even in unexpected states.
#
# Why all the || true?
# With 'set -e', any command returning non-zero causes immediate exit. In a trap handler,
# this can override a successful script exit (exit 0) with failure (exit 1). Examples:
#   - kill: Returns 1 if process doesn't exist (common if already finished)
#   - [[ test ]] && command: Returns 1 if test is false (breaks set -e)
#   - rm -f: Safe alone, but [[ test ]] && rm -f fails if test is false!
#
# The Fix: Add || true to every operation that might fail in normal operation.
#
# Real bug we hit: Script exited with code 1 despite successful backup because cleanup
# trap's `[[ -n "$VAR" ]] && rm -f "$VAR"` returned 1 when VAR was unset (analysis
# skipped). The `|| true` at the end ensures the line always succeeds.

cleanup() {
  # Kill background spinner processes
  # Why if-statements not && chains? More readable and explicitly handles each case.
  # PIDs may not exist if: script exited early, process already finished, or never started.
  if [[ -n "${ANALYSIS_SPINNER_PID:-}" ]]; then
    kill "$ANALYSIS_SPINNER_PID" 2>/dev/null || true  # May already be dead
  fi
  if [[ -n "${ARCHIVE_SPINNER_PID:-}" ]]; then
    kill "$ARCHIVE_SPINNER_PID" 2>/dev/null || true  # May already be dead
  fi

  # Remove temporary IPC files
  # Using if-statements instead of [[ test ]] && rm pattern to satisfy shellcheck SC2015.
  # While `[[ -n "$VAR" ]] && rm -f "$VAR" || true` works correctly here (the || true
  # ensures success regardless of test result), shellcheck warns that A && B || C is not
  # proper if-then-else. Using explicit if-statements is clearer and avoids the warning.
  if [[ -n "${ANALYSIS_RUNNING_FILE:-}" ]]; then
    rm -f "$ANALYSIS_RUNNING_FILE"
  fi
  if [[ -n "${ANALYSIS_COUNT_FILE:-}" ]]; then
    rm -f "$ANALYSIS_COUNT_FILE"
  fi
  if [[ -n "${ANALYSIS_SIZE_FILE:-}" ]]; then
    rm -f "$ANALYSIS_SIZE_FILE"
  fi
  if [[ -n "${PROGRESS_FILE:-}" ]]; then
    rm -f "$PROGRESS_FILE"
  fi

  # Clear progress line (cosmetic, can't fail)
  printf "\r\033[K"

  # Explicitly return success to preserve script's actual exit code
  # Without this, last command's exit code becomes cleanup's return value
  return 0
}

# Install cleanup trap for normal exit, interrupts (Ctrl-C), and termination signals
# This ensures temp files and background processes are always cleaned up
trap cleanup EXIT INT TERM

# ============================================================================
# Argument Parsing
# ============================================================================

parse_arguments() {
  while [[ $# -gt 0 ]]; do
    case $1 in
      -h|--help)
        usage
        exit 0
        ;;
      -n|--name)
        if [[ -z "${2:-}" ]]; then
          echo "Error: --name requires a name" >&2
          exit 1
        fi
        BACKUP_NAME="$2"
        shift 2
        ;;
      -d|--dest)
        if [[ -z "${2:-}" ]]; then
          echo "Error: --dest requires a directory path" >&2
          exit 1
        fi
        DEST_DIR="$2"
        shift 2
        ;;
      -a|--analyze)
        ANALYZE=true
        shift
        ;;
      --no-analyze)
        ANALYZE=false
        shift
        ;;
      --zstd)
        COMPRESSION="zstd"
        shift
        ;;
      --gzip)
        COMPRESSION="gzip"
        shift
        ;;
      --xz)
        COMPRESSION="xz"
        shift
        ;;
      --fast)
        # Optimize for speed: zstd level 1, no analysis
        COMPRESSION="zstd"
        COMPRESSION_LEVEL=1
        ANALYZE=false
        shift
        ;;
      --best)
        # Optimize for compression: zstd level 19
        COMPRESSION="zstd"
        COMPRESSION_LEVEL=19
        shift
        ;;
      -j|--threads)
        if [[ -z "${2:-}" ]]; then
          echo "Error: --threads requires a number" >&2
          exit 1
        fi
        COMPRESSION_THREADS="$2"
        shift 2
        ;;
      -*)
        echo "Error: Unknown option: $1" >&2
        usage
        exit 1
        ;;
      *)
        DIRS_TO_BACKUP+=("$1")
        shift
        ;;
    esac
  done

  # Show usage if no directories specified
  if [[ ${#DIRS_TO_BACKUP[@]} -eq 0 ]]; then
    usage
    exit 0
  fi
}

# ============================================================================
# Directory Validation
# ============================================================================

validate_directories() {
  local existing_dirs=()
  local missing_dirs=()
  local dir_idx=0

  echo ""
  print_banner "Backup Directories" "orange"

  # Check each directory
  for dir in "${DIRS_TO_BACKUP[@]}"; do
    # Normalize path: if absolute, use as-is; if relative, prepend HOME
    local full_path
    if [[ "$dir" = /* ]]; then
      # Absolute path - use as-is
      full_path="$dir"
    else
      # Relative path - prepend HOME
      full_path="$HOME/$dir"
    fi

    # Convert to relative path from HOME for tar (if inside HOME)
    local relative_dir="$dir"
    if [[ "$dir" = /* ]]; then
      # If absolute path starts with HOME, make it relative
      if [[ "$dir" = "$HOME"* ]]; then
        relative_dir="${dir#"$HOME"/}"
      else
        # Outside HOME - keep absolute
        relative_dir="$dir"
      fi
    fi

    if [[ -e "$full_path" ]]; then
      existing_dirs+=("$relative_dir")
      local color_code="${DIR_COLORS[$dir_idx]}"
      echo -e "  ✓ Found: \033[${color_code}m$full_path\033[0m"
      dir_idx=$(( (dir_idx + 1) % ${#DIR_COLORS[@]} ))
    else
      missing_dirs+=("$relative_dir")
      echo -e "  ⊘ Skipping: \033[90m$full_path\033[0m \033[33m(not found)\033[0m"
    fi
  done

  echo ""

  # Check if we have anything to backup
  if [[ ${#existing_dirs[@]} -eq 0 ]]; then
    color_red "Error: No directories found to backup"
    exit 1
  fi

  # Check if destination directory exists
  if [[ ! -d "$DEST_DIR" ]]; then
    color_red "Error: Destination directory not found: $DEST_DIR"
    exit 1
  fi

  # Return the lists via arrays (caller reads from these globals)
  EXISTING_DIRS=("${existing_dirs[@]}")
  if [[ ${#missing_dirs[@]} -gt 0 ]]; then
    MISSING_DIRS=("${missing_dirs[@]}")
  else
    MISSING_DIRS=()
  fi
}

# ============================================================================
# Analysis Phase - Count files and estimate size
# ============================================================================

analyze_directories() {
  local existing_dirs=("$@")

  color_blue "Analyzing directories..."

  local estimated_files=0
  local estimated_dirs=0
  local estimated_size=0
  local spinner_idx=0

  # Set up temp files for IPC
  ANALYSIS_COUNT_FILE="/tmp/backup_analysis_count_$$"
  ANALYSIS_SIZE_FILE="/tmp/backup_analysis_size_$$"
  ANALYSIS_RUNNING_FILE="/tmp/backup_analysis_running_$$"

  echo "0" > "$ANALYSIS_COUNT_FILE"
  echo "0" > "$ANALYSIS_SIZE_FILE"
  touch "$ANALYSIS_RUNNING_FILE"

  # Start background spinner for live progress display
  set +m  # Disable job control messages
  (
    while [[ -f "$ANALYSIS_RUNNING_FILE" ]]; do
      local count=$(cat "$ANALYSIS_COUNT_FILE" 2>/dev/null || echo "0")
      local size=$(cat "$ANALYSIS_SIZE_FILE" 2>/dev/null || echo "0")
      local size_hr=$(numfmt --to=iec-i --suffix=B "$size" 2>/dev/null || echo "0")
      local spinner_char="${SPINNER_CHARS:$spinner_idx:1}"

      # Rainbow color based on count
      local color_idx=$(( (count / COLOR_CYCLE_FILES) % ${#COLORS[@]} ))
      local color_code="${COLORS[$color_idx]}"

      printf "\r  \033[96m%s\033[0m \033[95mScanning...\033[0m \033[${color_code}m%'d\033[0m \033[93mentries\033[0m (\033[92m%s\033[0m)    " \
        "$spinner_char" "$count" "$size_hr"
      spinner_idx=$(( (spinner_idx + 1) % ${#SPINNER_CHARS} ))
      sleep "$ANALYSIS_SPINNER_SLEEP"
    done
  ) &
  ANALYSIS_SPINNER_PID=$!
  disown 2>/dev/null || true  # Prevent job control messages (allow to fail silently)
  set -m  # Re-enable job control

  # Build exclude arguments for fd
  local exclude_args=()
  for pattern in "${EXCLUDE_PATTERNS[@]}"; do
    exclude_args+=("--exclude" "$pattern")
  done

  # Count files and size using fast fd command
  local last_update=$(date +%s%N)
  local update_interval=$UPDATE_INTERVAL_NS

  for dir in "${existing_dirs[@]}"; do
    # Normalize path: if relative, prepend HOME; if absolute, use as-is
    local search_path
    if [[ "$dir" = /* ]]; then
      search_path="$dir"
    else
      search_path="$HOME/$dir"
    fi

    while IFS= read -r file; do
      ((estimated_files++))
      # Get file size using wc -c (cross-platform)
      # GOTCHA: Don't use `stat --format='%s' "$file"` - it returns exit code 1 on macOS
      # even though it works correctly! This breaks scripts with 'set -e'. The wc -c
      # approach works reliably on all platforms with consistent exit codes.
      local size=$(wc -c < "$file" 2>/dev/null || echo 0)
      estimated_size=$((estimated_size + size))

      # Update progress based on time interval for organic feel
      local now=$(date +%s%N)
      local elapsed=$((now - last_update))
      if [[ $elapsed -ge $update_interval ]]; then
        echo "$estimated_files" > "$ANALYSIS_COUNT_FILE"
        echo "$estimated_size" > "$ANALYSIS_SIZE_FILE"
        last_update=$now
      fi
    done < <(fd --type f \
      --no-ignore \
      --hidden \
      "${exclude_args[@]}" \
      --absolute-path \
      . "$search_path" 2>/dev/null)
  done

  # Count directories (tar counts these too)
  for dir in "${existing_dirs[@]}"; do
    # Normalize path
    local search_path
    if [[ "$dir" = /* ]]; then
      search_path="$dir"
    else
      search_path="$HOME/$dir"
    fi

    while IFS= read -r _dir; do
      ((estimated_dirs++))
      # Update progress to show total count (files + dirs)
      local now=$(date +%s%N)
      local elapsed=$((now - last_update))
      if [[ $elapsed -ge $update_interval ]]; then
        echo "$((estimated_files + estimated_dirs))" > "$ANALYSIS_COUNT_FILE"
        last_update=$now
      fi
    done < <(fd --type d \
      --no-ignore \
      --hidden \
      "${exclude_args[@]}" \
      --absolute-path \
      . "$search_path" 2>/dev/null)
  done

  # Final update with total count
  local total_entries=$((estimated_files + estimated_dirs))
  echo "$total_entries" > "$ANALYSIS_COUNT_FILE"
  echo "$estimated_size" > "$ANALYSIS_SIZE_FILE"
  sleep 0.2  # Let spinner show final values

  # Stop analysis spinner and cleanup
  rm -f "$ANALYSIS_RUNNING_FILE"
  wait "$ANALYSIS_SPINNER_PID" 2>/dev/null || true  # Don't fail if process already exited
  rm -f "$ANALYSIS_COUNT_FILE" "$ANALYSIS_SIZE_FILE"

  # Clear the spinner line and print analysis summary
  printf "\r\033[K"
  local size_hr=$(numfmt --to=iec-i --suffix=B "$estimated_size" 2>/dev/null || echo "0")
  local entries_formatted=$(printf "%'d" "$total_entries")
  echo "  $(color_bright_green "✓") Analysis complete: $(color_bright_cyan "$entries_formatted") entries found ($(color_bright_yellow "$size_hr"))"
  echo ""

  # Return values via globals (total entries = files + dirs, just like tar)
  ESTIMATED_FILES=$total_entries
  ESTIMATED_SIZE=$estimated_size
}

# ============================================================================
# Progress Display Functions (Modular, No Flag Mixing)
# ============================================================================

# Progress display WITH percentage (requires ESTIMATED_FILES to be set)
start_progress_with_percentage() {
  local progress_file="$1"
  local start_time="$2"

  set +m  # Disable job control messages
  (
    local spinner_idx=0
    while [[ -f "$progress_file" ]]; do
      if [[ -r "$progress_file" ]]; then
        local count=$(cat "$progress_file" 2>/dev/null || echo "0")

        # Calculate elapsed time
        local now=$(date +%s)
        local elapsed=$((now - start_time))
        local time_str=$(printf "%02d:%02d" $((elapsed/60)) $((elapsed%60)))

        # Calculate percentage (capped at 100%)
        local percent=0
        if [[ $ESTIMATED_FILES -gt 0 ]]; then
          percent=$((count * 100 / ESTIMATED_FILES))
          [[ $percent -gt 100 ]] && percent=100
        fi

        # Rainbow color based on count
        local color_idx=$(( (count / COLOR_CYCLE_FILES) % ${#COLORS[@]} ))
        local color_code="${COLORS[$color_idx]}"

        # Color for percentage (green if 100%, cyan otherwise)
        local percent_color="96"  # Cyan
        [[ $percent -eq 100 ]] && percent_color="92"  # Green

        local spinner_char="${SPINNER_CHARS:$spinner_idx:1}"
        printf "\r  \033[96m%s\033[0m [\033[93m%s\033[0m] \033[95mProcessing...\033[0m \033[${color_code}m%'d\033[0m \033[93mentries\033[0m (\033[${percent_color}m%d%%\033[0m)    " \
          "$spinner_char" "$time_str" "$count" "$percent"
        spinner_idx=$(( (spinner_idx + 1) % ${#SPINNER_CHARS} ))
      fi
      sleep "$ARCHIVE_SPINNER_SLEEP"
    done
  ) &
  ARCHIVE_SPINNER_PID=$!
  disown 2>/dev/null || true  # Prevent job control messages (allow to fail silently)
  set -m  # Re-enable job control
}

# Progress display WITHOUT percentage (fast mode, no analysis needed)
start_progress_count_only() {
  local progress_file="$1"
  local start_time="$2"

  set +m  # Disable job control messages
  (
    local spinner_idx=0
    while [[ -f "$progress_file" ]]; do
      if [[ -r "$progress_file" ]]; then
        local count=$(cat "$progress_file" 2>/dev/null || echo "0")

        # Calculate elapsed time
        local now=$(date +%s)
        local elapsed=$((now - start_time))
        local time_str=$(printf "%02d:%02d" $((elapsed/60)) $((elapsed%60)))

        # Rainbow color based on count
        local color_idx=$(( (count / COLOR_CYCLE_FILES) % ${#COLORS[@]} ))
        local color_code="${COLORS[$color_idx]}"

        local spinner_char="${SPINNER_CHARS:$spinner_idx:1}"
        # No percentage - just count and time
        printf "\r  \033[96m%s\033[0m [\033[93m%s\033[0m] \033[95mProcessing...\033[0m \033[${color_code}m%'d\033[0m \033[93mentries\033[0m    " \
          "$spinner_char" "$time_str" "$count"
        spinner_idx=$(( (spinner_idx + 1) % ${#SPINNER_CHARS} ))
      fi
      sleep "$ARCHIVE_SPINNER_SLEEP"
    done
  ) &
  ARCHIVE_SPINNER_PID=$!
  disown 2>/dev/null || true  # Prevent job control messages (allow to fail silently)
  set -m  # Re-enable job control
}

# ============================================================================
# Archive Phase - Create compressed archive with progress
# ============================================================================

create_archive() {
  local backup_dest="$1"
  shift
  local existing_dirs=("$@")

  print_info "Creating backup archive..."
  echo "  Destination: $(color_bright_magenta "$backup_dest")"
  echo ""

  # Start timing
  ARCHIVE_START_TIME=$(date +%s)

  # Determine working directory for tar:
  # - If all paths are relative or inside HOME: cd to HOME, use relative paths
  # - If any path is absolute outside HOME: use absolute paths from current dir
  local use_home=true
  local tar_dirs=()

  for dir in "${existing_dirs[@]}"; do
    if [[ "$dir" = /* ]]; then
      # Absolute path - check if it's inside HOME
      if [[ "$dir" != "$HOME"/* ]]; then
        use_home=false
      fi
    fi
  done

  if [[ "$use_home" == true ]]; then
    # All paths are relative or inside HOME - use HOME as base
    cd "$HOME" || exit 1
    for dir in "${existing_dirs[@]}"; do
      if [[ "$dir" = /* ]]; then
        # Absolute path inside HOME - make relative
        tar_dirs+=("${dir#"$HOME"/}")
      else
        # Already relative
        tar_dirs+=("$dir")
      fi
    done
  else
    # Has paths outside HOME - use absolute paths
    for dir in "${existing_dirs[@]}"; do
      if [[ "$dir" = /* ]]; then
        tar_dirs+=("$dir")
      else
        tar_dirs+=("$HOME/$dir")
      fi
    done
  fi

  # Initialize progress tracking
  local processed=0
  local spinner_idx=0
  PROGRESS_FILE="/tmp/backup_dirs_$$_progress"

  echo "0" > "$PROGRESS_FILE"

  # Start appropriate progress display based on ANALYZE flag
  # Clean branching - no flag logic mixed into display code
  if [[ $ANALYZE == true ]]; then
    start_progress_with_percentage "$PROGRESS_FILE" "$ARCHIVE_START_TIME"
  else
    start_progress_count_only "$PROGRESS_FILE" "$ARCHIVE_START_TIME"
  fi

  # Build tar exclude arguments
  local exclude_args=()
  for pattern in "${EXCLUDE_PATTERNS[@]}"; do
    exclude_args+=("--exclude=$pattern")
  done

  # ==========================================================================
  # Archive Creation - The Critical Section
  # ==========================================================================
  #
  # CRITICAL CONCEPT: Separating tar's archive data from verbose output
  #
  # Problem: We want to:
  #   1. Pipe tar's archive data through zstd to file
  #   2. Count tar's verbose output (one line per file) for progress
  #   3. Do both simultaneously without corrupting the archive
  #
  # tar's I/O streams:
  #   stdout: Archive data (binary tar stream)
  #   stderr: Verbose output with -v flag (one filename per line)
  #
  # Solution Pattern (for zstd):
  #   {
  #     tar -cv ... | zstd ... > file    # Archive pipeline in grouped command
  #   } 2>&1 | {                          # Redirect stderr (verbose) to pipeline
  #     count lines                       # Count filenames for progress
  #   }
  #
  # Why the curly braces?
  #   Grouping { commands; } lets us redirect stderr of the ENTIRE pipeline.
  #   Without grouping, `tar | zstd 2>&1` would mix tar's verbose with zstd's output.
  #
  # Why `set +o pipefail`?
  #   tar returns exit code 1 for warnings (e.g., "file changed while reading")
  #   even when archive is successfully created. With pipefail, this would fail
  #   the entire script. We check archive validity after creation instead.
  #
  # Real bug we fixed: Initial attempt used complex file descriptor manipulation
  #   `tar 2>&1 >&3 3>&- | zstd` which created corrupted archives. The grouped
  #   command approach is simpler and actually works.

  set +o pipefail  # Disable pipefail: tar warnings shouldn't fail the script
  local last_update=$(date +%s%N)
  local update_interval=$UPDATE_INTERVAL_NS

  case $COMPRESSION in
    zstd)
      # Multi-threaded zstd - The Modern Default
      # Why zstd? 2-3x faster than gzip, better compression, multi-threaded.
      # -T0 = auto-detect cores (uses all available CPU)
      # -3 = level 3 (balanced speed/ratio, can be 1-19)
      {
        tar -cv "${exclude_args[@]}" "${tar_dirs[@]}" | \
          zstd -T"$COMPRESSION_THREADS" -"$COMPRESSION_LEVEL" > "$backup_dest"
      } 2>&1 | \
      {
        # Count loop: Read tar's verbose output (filenames) and update progress file
        local processed=0
        local last_update=$(date +%s%N)
        while IFS= read -r _line; do
          ((processed++))
          # Throttle updates to UPDATE_INTERVAL_NS (90ms) to avoid thrashing
          local now=$(date +%s%N)
          if [[ $((now - last_update)) -ge $update_interval ]]; then
            echo "$processed" > "$PROGRESS_FILE"  # Background spinner reads this
            last_update=$now
          fi
        done
        echo "$processed" > "$PROGRESS_FILE"  # Final count
      }
      ;;
    gzip)
      # Traditional gzip (compatible)
      tar -czf "$backup_dest" "${exclude_args[@]}" -v "${tar_dirs[@]}" 2>&1 | \
      {
        local processed=0
        local last_update=$(date +%s%N)
        while IFS= read -r _line; do
          ((processed++))
          local now=$(date +%s%N)
          if [[ $((now - last_update)) -ge $update_interval ]]; then
            echo "$processed" > "$PROGRESS_FILE"
            last_update=$now
          fi
        done
        echo "$processed" > "$PROGRESS_FILE"
      }
      ;;
    xz)
      # Best compression (slow)
      tar -cJf "$backup_dest" "${exclude_args[@]}" -v "${tar_dirs[@]}" 2>&1 | \
      {
        local processed=0
        local last_update=$(date +%s%N)
        while IFS= read -r _line; do
          ((processed++))
          local now=$(date +%s%N)
          if [[ $((now - last_update)) -ge $update_interval ]]; then
            echo "$processed" > "$PROGRESS_FILE"
            last_update=$now
          fi
        done
        echo "$processed" > "$PROGRESS_FILE"
      }
      ;;
  esac

  # Re-enable pipefail now that archiving is complete
  set -o pipefail

  # ==========================================================================
  # Post-Archive Cleanup and Validation
  # ==========================================================================

  # Read final file count BEFORE deleting the progress file
  # The count loop writes final value after tar completes, but there's a race condition
  # where we might read before the final write. Brief sleep ensures we get accurate count.
  sleep 0.1  # Let final echo "$processed" > "$PROGRESS_FILE" complete
  PROCESSED_FILES=$(cat "$PROGRESS_FILE" 2>/dev/null || echo "0")

  # Stop the background spinner process
  rm -f "$PROGRESS_FILE"  # Spinner checks for this file, removing it stops the loop

  # Wait for spinner to exit cleanly
  # CRITICAL GOTCHA: wait returns non-zero if process already exited or doesn't exist!
  # With 'set -e', this would cause function to return 1 despite successful archive.
  # The '|| true' ensures wait failure doesn't fail the function.
  wait "$ARCHIVE_SPINNER_PID" 2>/dev/null || true  # Don't fail if already exited

  # Clear the progress line (cosmetic cleanup)
  printf "\r\033[K"

  # Archive validation - More reliable than checking tar's exit code
  # Why not trust tar's exit code? tar returns 1 for non-fatal warnings like
  # "file changed while reading" even when archive is valid and complete.
  # Instead, verify the file exists and has content (not 0 bytes).
  if [[ ! -f "$backup_dest" ]] || [[ ! -s "$backup_dest" ]]; then
    return 1  # Archive creation failed
  fi

  return 0  # Success!
}

# ============================================================================
# Statistics Display
# ============================================================================

display_statistics() {
  local backup_dest="$1"
  local num_dirs="$2"
  local num_missing="$3"

  # Calculate timing
  local end_time=$(date +%s)
  local analysis_elapsed=$((ARCHIVE_START_TIME - OVERALL_START_TIME))
  local archive_elapsed=$((end_time - ARCHIVE_START_TIME))
  local total_elapsed=$((end_time - OVERALL_START_TIME))

  local analysis_time_str=$(printf "%02d:%02d" $((analysis_elapsed / 60)) $((analysis_elapsed % 60)))
  local archive_time_str=$(printf "%02d:%02d" $((archive_elapsed / 60)) $((archive_elapsed % 60)))
  local total_time_str=$(printf "%02d:%02d" $((total_elapsed / 60)) $((total_elapsed % 60)))

  # Get compressed archive size (using wc -c for cross-platform reliability)
  # tr -d ' ' removes any leading spaces that wc might output on some platforms
  local compressed_size=$(wc -c < "$backup_dest" 2>/dev/null | tr -d ' ' || echo 0)
  local compressed_size_hr=$(numfmt --to=iec-i --suffix=B "$compressed_size" 2>/dev/null || echo "$((compressed_size / 1048576))M")
  local estimated_size_hr=$(numfmt --to=iec-i --suffix=B "$ESTIMATED_SIZE" 2>/dev/null || echo "$((ESTIMATED_SIZE / 1048576))M")

  # Calculate compression ratio
  local compression_percent=0
  if [[ $ESTIMATED_SIZE -gt 0 ]]; then
    compression_percent=$(( (ESTIMATED_SIZE - compressed_size) * 100 / ESTIMATED_SIZE ))
  fi

  # Display statistics
  echo ""
  print_section "Backup Statistics" "brightcyan"
  echo ""
  echo "  Location:        $(color_bright_blue "$backup_dest")"

  # Show entries with estimate only if analysis was run
  if [[ $ANALYZE == true && $ESTIMATED_FILES -gt 0 ]]; then
    echo "  Entries:         $(color_bright_green "$PROCESSED_FILES") $(color_bright_black "(estimated: $ESTIMATED_FILES)")"
    local accuracy=$(( PROCESSED_FILES * 100 / ESTIMATED_FILES ))
    echo "  Accuracy:        $(color_bright_cyan "$accuracy%")"
  else
    echo "  Entries:         $(color_bright_green "$PROCESSED_FILES")"
  fi

  # Show size estimates only if analysis was run
  if [[ $ANALYZE == true && $ESTIMATED_SIZE -gt 0 ]]; then
    echo "  Original size:   $(color_bright_yellow "$estimated_size_hr")"
    echo "  Compressed size: $(color_bright_green "$compressed_size_hr")"
    echo "  Compression:     $(color_bright_magenta "${compression_percent}%") saved"
  else
    echo "  Compressed size: $(color_bright_green "$compressed_size_hr")"
  fi

  echo "  Directories:     $(color_bright_green "$num_dirs") backed up"
  echo ""

  # Show timing with analysis breakdown only if analysis was run
  if [[ $ANALYZE == true ]]; then
    echo "  Time - Analysis: $(color_bright_cyan "$analysis_time_str")"
    echo "  Time - Archive:  $(color_bright_cyan "$archive_time_str")"
    echo "  Time - Total:    $(color_bright_green "$total_time_str")"
  else
    echo "  Time - Total:    $(color_bright_green "$total_time_str")"
  fi

  if [[ $num_missing -gt 0 ]]; then
    echo "  Skipped:         $(color_orange "$num_missing")"
  fi

  echo ""
  print_section "Notes" "brightyellow"
  echo ""
  echo "  $(color_cyan "•") Excludes: $(color_bright_black "${EXCLUDE_PATTERNS[*]:0:3}, ...")"
  echo "  $(color_cyan "•") Git history $(color_bright_black "(.git)") included for recovery"
  echo "  $(color_cyan "•") Backup saved to: $(color_bright_blue "$DEST_DIR")"
  echo ""

  # Success message
  echo ""
  echo -e "  \033[92m✓ Backup completed successfully!\033[0m"
  echo ""
}

# ============================================================================
# Main
# ============================================================================

main() {
  # Start timing
  OVERALL_START_TIME=$(date +%s)

  # Parse command line arguments
  parse_arguments "$@"

  # Validate directories exist and build lists
  validate_directories

  # Create backup filename with correct extension for compression type
  local date_stamp=$(date '+%Y-%m-%d_%H%M%S')
  local extension="${COMPRESSION_EXTENSIONS[$COMPRESSION]}"
  local backup_name="${BACKUP_NAME}_${date_stamp}.${extension}"
  local backup_dest="$DEST_DIR/$backup_name"

  # Analysis phase - count files and estimate size (OPTIONAL - skipped by default for speed)
  if [[ $ANALYZE == true ]]; then
    analyze_directories "${EXISTING_DIRS[@]}"
  else
    # Skip analysis - no ESTIMATED_FILES or ESTIMATED_SIZE set
    # Progress will show raw count without percentage
    ESTIMATED_FILES=0
    ESTIMATED_SIZE=0
  fi

  # Archive phase - create compressed backup
  if create_archive "$backup_dest" "${EXISTING_DIRS[@]}"; then
    # Display final statistics
    display_statistics "$backup_dest" "${#EXISTING_DIRS[@]}" "${#MISSING_DIRS[@]}"
    exit 0
  else
    echo ""
    color_red "✗ Backup failed"
    exit 1
  fi
}

# Run main if executed directly
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
  main "$@"
fi
