# Agent Metrics Extraction - Implementation Status

**Date**: 2025-12-05
**Status**: âœ… Phase 1 Complete - Extraction automated via PostToolUse hook
**Next**: Analysis moved to separate ccm project (`~/code/claude-code-metrics`)

## Current State

**Extraction (Dotfiles)**: âœ… Complete

- PostToolUse hook registered for Task tool (`.claude/settings.json`)
- Hook wrapper extracts agentId from tool_response (`.claude/hooks/post-task-extract-metrics`)
- Metrics extraction library parses agent transcripts (`.claude/lib/extract_agent_metrics.py`)
- Comprehensive test suite validates extraction (`.claude/tests/test_extract_agent_metrics.py`)
- Outputs to `.claude/metrics/commit-metrics-{date}.jsonl`

**Analysis (CCM Project)**: ðŸš§ In Progress

- Moved to separate project: `~/code/claude-code-metrics`
- See ccm project for Phase 2 implementation (DuckDB + Parquet + Sweetviz)
- Research document: `~/code/claude-code-metrics/docs/research/metrics-analysis-research.md`
- Implementation plan: `~/code/claude-code-metrics/.planning/implementation-plan.md`

## Separation of Concerns

**Dotfiles Scope** (Data Collection):

- PostToolUse hook registration
- Metrics extraction from transcripts
- Output: JSONL files in `.claude/metrics/`
- Lightweight, fast, no heavy dependencies

**CCM Project Scope** (Data Analysis):

- Read JSONL from dotfiles metrics directory
- Convert to Parquet for efficient analytics
- DuckDB query library for common analyses
- Sweetviz/D-Tale automated reporting
- Jupyter notebooks for exploration
- Location: `~/code/claude-code-metrics`

**Data Flow**:

```text
Claude Code Agent â†’ Transcript â†’ PostToolUse Hook â†’ extract_agent_metrics.py
                                                              â†“
                                                    .claude/metrics/*.jsonl
                                                              â†“
                                                    CCM Incremental Loader
                                                              â†“
                                        ~/code/claude-code-metrics/data/*.parquet
                                                              â†“
                                                DuckDB Queries + Sweetviz Reports
```

---

## Critical Discovery: Hook Context and Transcript Relationships

### What We Know Now

**Hook Behavior**:
- PostToolUse hooks in subagents receive **parent session context**
- Hook provides: `session_id` (parent), `transcript_path` (parent)
- Hook does NOT provide: `agentId`, agent transcript path

**Parent Transcript Contains**:
- All subagent tool calls and results
- Agent ID embedded in `toolUseResult` blocks
- Full usage metrics per message (tokens, cache stats)
- Thinking blocks (if any)
- Complete conversation flow

**Agent Transcript Contains** (e.g., `agent-b59f7ef4.jsonl`):
- Same tool calls and results as parent
- Isolated to just that agent's work
- Marked with `isSidechain: true`
- All the same metrics but in smaller file
- Structure: ~30 lines for simple commit vs ~90k lines parent

**Example Structure Comparison**:

```json
// Parent transcript entry (line 5809)
{
  "parentUuid": "...",
  "sessionId": "dac1b79c-1435-4fa0-8d76-b4fd3a5b4a4e",
  "type": "user",
  "toolUseResult": {
    "status": "completed",
    "agentId": "b59f7ef4",  // <- Key identifier
    "content": [...],
    "totalDurationMs": 67362,
    "totalTokens": 17604,
    "totalToolUseCount": 10,
    "usage": { "input_tokens": 5, "cache_read_input_tokens": 17387, ... }
  }
}

// Agent transcript entry (line 0)
{
  "parentUuid": null,
  "isSidechain": true,  // <- Marks as agent
  "sessionId": "dac1b79c-1435-4fa0-8d76-b4fd3a5b4a4e",
  "agentId": "b59f7ef4",  // <- Available in first line
  "slug": "agile-giggling-willow",
  "type": "assistant",
  "usage": { "input_tokens": 3, "cache_creation_input_tokens": 5479, ... }
}
```

---

## Metrics We Can Extract

### From Both Transcripts (Available)

#### Token Metrics

- `input_tokens` - Tokens read from context
- `cache_creation_input_tokens` - New tokens added to cache
- `cache_read_input_tokens` - Tokens read from cache (cheaper)
- `output_tokens` - Tokens generated by model
- `service_tier` - "standard" or other tier
- **Calculated**: Total tokens per agent run
- **Calculated**: Cache hit rate
- **Calculated**: Cost estimation (with rate card)

#### Execution Metrics

- `totalDurationMs` - Full agent execution time (from toolUseResult)
- Per-tool timing (can extract from message timestamps)
- Tool count by type (Bash, Read, Edit, etc.)
- Total tool uses

#### Git/Commit Metrics (Commit Agent Specific)

- Commits created (count)
- Commit hashes (array)
- Files changed (parse from git commands)
- Pre-commit iterations (count logsift runs)
- Pre-commit failures/successes

#### Quality Indicators

- Error occurrences (grep transcript for error patterns)
- Retry patterns (multiple calls to same tool)
- Phase execution (commit-agent: did Phase 4, 5, 7 run?)
- Instruction compliance (did agent read own instructions?)

### Additional Context Available

#### Session Metadata

- `session_id` - Parent session
- `agentId` - Unique agent identifier
- `slug` - Human-readable session name
- `cwd` - Working directory
- `gitBranch` - Current branch
- `version` - Claude Code version

#### Model Details

- `model` - Exact model ID (e.g., "claude-sonnet-4-5-20250929")
- `requestId` - API request identifier
- `stop_reason` - How response ended (tool_use, end_turn, etc.)

---

## Approach Options

### Option 1: Parse Parent Transcript Directly

**How It Works**:
1. Hook provides parent transcript path
2. Read entire parent transcript (or tail -n for recent entries)
3. Filter for most recent `toolUseResult` with an `agentId`
4. Extract metrics from that block and surrounding context

**Advantages**:
- âœ… Single file to read (from hook)
- âœ… All data in one place
- âœ… No need to construct agent transcript path
- âœ… Contains relationship to parent session

**Disadvantages**:
- âŒ Large file to parse (~90k tokens, though JSONL is line-based)
- âŒ Mixed content (parent + all subagents)
- âŒ Need to filter for specific agent's activity
- âŒ Performance overhead for large sessions

**Implementation Complexity**: Medium

**Best For**: Quick prototypes, when parent context matters

---

### Option 2: Extract AgentId â†’ Read Agent Transcript

**How It Works**:
1. Hook provides parent transcript path
2. Tail parent transcript, grep for most recent `"agentId"`
3. Extract agent ID (e.g., "b59f7ef4")
4. Construct agent transcript path: `~/.claude/projects/{project}/agent-{agentId}.jsonl`
5. Read agent transcript (much smaller ~30 lines)
6. Parse metrics

**Advantages**:
- âœ… Small file to parse (agent transcript is focused)
- âœ… Clean separation (only agent activity)
- âœ… Faster parsing
- âœ… Easy to read entire file without worrying about size

**Disadvantages**:
- âŒ Two-step process (find agentId, then read agent file)
- âŒ Assumes agent transcript exists (it should for Task tool agents)
- âŒ Need to know project path structure

**Implementation Complexity**: Medium

**Best For**: Production metrics collection, long-term scalability

---

### Option 3: Hybrid - Agent Transcript with Parent Fallback

**How It Works**:
1. Try to find and read agent transcript (Option 2)
2. If agent transcript missing or incomplete, fall back to parent (Option 1)
3. Extract metrics from whichever source is available

**Advantages**:
- âœ… Robust to different execution contexts
- âœ… Works for both agents and main sessions
- âœ… Future-proof

**Disadvantages**:
- âŒ More complex logic
- âŒ Harder to test
- âŒ Two code paths to maintain

**Implementation Complexity**: High

**Best For**: Universal metrics library for all agent types

---

## Python vs Bash Comparison

### Python Advantages

**JSON Parsing**:
```python
import json
for line in open(transcript_path):
    entry = json.loads(line)
    if entry.get("agentId"):
        print(f"Found agent: {entry['agentId']}")
```

**Data Structures**:
```python
# Easy aggregation
metrics = {
    "total_tokens": 0,
    "tool_uses": defaultdict(int),
    "cache_hits": 0
}

for line in transcript:
    msg = json.loads(line)
    usage = msg.get("message", {}).get("usage", {})
    metrics["total_tokens"] += usage.get("input_tokens", 0)
```

**Type Safety**:
```python
from dataclasses import dataclass
from typing import Optional

@dataclass
class AgentMetrics:
    agent_id: str
    session_id: str
    total_tokens: int
    duration_ms: Optional[int] = None
```

**Libraries**:
- `datetime` for timestamp parsing
- `pathlib` for file operations
- `argparse` for CLI
- `statistics` for calculations
- `rich` for pretty output

**Error Handling**:
```python
try:
    metrics = extract_metrics(transcript_path)
except JSONDecodeError:
    logger.error("Corrupt transcript")
    return None
```

**Testing**:
- Unit tests with pytest
- Mock file I/O
- Fixtures for sample transcripts

### Bash Advantages

**Performance**:
```bash
# Extract agentId from last 100 lines (fast)
tail -100 "$PARENT_TRANSCRIPT" | grep -m1 '"agentId"' | jq -r '.agentId'
```

**Simplicity**:
```bash
# One-liner for total tokens
jq -s 'map(.message.usage.input_tokens // 0) | add' agent.jsonl
```

**Integration**:
- Already used in `.claude/lib/log-commit-metrics.sh`
- Works directly in agent instructions
- No Python environment needed

**Streaming**:
```bash
# Process large files line-by-line without loading into memory
while IFS= read -r line; do
    jq -r '.agentId' <<< "$line"
done < transcript.jsonl
```

### Recommendation: **Python for Metrics Extraction**

**Rationale**:
1. **Complex Data Processing**: Metrics involve aggregations, calculations, correlations
2. **Type Safety**: Dataclasses prevent errors in metric structure
3. **Maintainability**: Easier to extend with new metrics
4. **Testing**: Better tooling for validation
5. **Error Handling**: Robust handling of corrupt/incomplete transcripts

**Use Bash For**:
- Quick agentId extraction (called from Python if needed)
- One-off debugging queries
- Shell integration in hooks

**Hybrid Approach**:
```python
# Python script can shell out for specific operations
import subprocess

def get_agent_id_fast(parent_transcript: Path) -> str:
    """Use bash for fast agentId extraction."""
    cmd = f"tail -100 {parent_transcript} | grep -m1 '\"agentId\"' | jq -r '.agentId'"
    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
    return result.stdout.strip()
```

---

## What Claude Code Already Offers

### Built-in OpenTelemetry Support

**Enable Telemetry**:
```bash
export CLAUDE_CODE_ENABLE_TELEMETRY=1
export OTEL_LOGS_EXPORTER=otlp
export OTEL_METRICS_EXPORTER=otlp
export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317
```

**Metrics Exported**:
- `claude_code.token.usage` - Token breakdown by type
- `claude_code.api_request` - Request duration and latency
- `claude_code.tool_result` - Tool performance
- `claude_code.active_time.total` - Actual usage time

**What's Included**:
- Prompt length (NOT content - privacy-safe)
- Token counts (input, output, cached)
- Model name and version
- Tool usage patterns
- Cost estimates
- Session duration

**What's NOT Included**:
- Actual prompts or responses (privacy/security)
- File contents
- API keys or credentials
- Working directory paths (sometimes included in cwd field)

### Observability Platforms

**SigNoz** ([Guide](https://signoz.io/blog/claude-code-monitoring-with-opentelemetry/)):
- Open-source APM
- Built on OpenTelemetry
- Pre-built dashboards for Claude Code
- Free tier available

**Honeycomb** ([Article](https://www.honeycomb.io/blog/can-claude-code-observe-its-own-code)):
- Commercial observability platform
- Powerful query language
- High-cardinality analysis

**Grafana Cloud** ([Guide](https://quesma.com/blog/track-claude-code-usage-and-limits-with-grafana-cloud/)):
- Prometheus + Loki + Tempo stack
- Claude Code OTEL integration guide
- Visualizations for tokens, costs, limits

**Datadog LLM Observability** ([Docs](https://docs.datadoghq.com/llm_observability/)):
- Enterprise-grade monitoring
- Integrated with existing Datadog infrastructure
- Cost tracking and optimization

### Community Tools

**claude-code-otel** ([GitHub](https://github.com/ColeMurray/claude-code-otel)):
- Comprehensive observability solution
- Docker Compose setup with Grafana/Prometheus
- Pre-built dashboards

**claude_telemetry** ([GitHub](https://github.com/TechNickAI/claude_telemetry)):
- CLI wrapper: `claudia` command replaces `claude`
- Logs to Logfire, Sentry, Honeycomb, Datadog
- Drop-in replacement

**Dev-Agent-Lens** ([Arize](https://arize.com/blog/claude-code-observability-and-tracing-introducing-dev-agent-lens/)):
- Specialized for agent observability
- Tracing for multi-step workflows
- Prompt/response analysis

---

## What Others Are Using

### Industry Best Practices

**OpenLLMetry** ([Traceloop](https://www.traceloop.com/blog/openllmetry)):
- OpenTelemetry extension for LLMs
- Instruments Anthropic, OpenAI, Cohere
- Vector DB integration (Pinecone, Weaviate)
- Framework support (LangChain, Haystack)

**Key Metrics Tracked**:
1. **Token Usage**: Direct cost driver, measure response complexity
2. **Request Latency**: User experience, performance optimization
3. **Error Rates**: Reliability, debugging
4. **Cost Per Request**: Budget management
5. **Cache Hit Rate**: Efficiency, cost reduction

**Langfuse** ([Docs](https://langfuse.com/integrations/native/opentelemetry)):
- Built on OpenTelemetry
- LLM-specific features:
  - Prompt versioning and linking
  - Token usage and cost tracking
  - Evaluation scores
  - Trace analysis

**OpenLIT** ([Docs](https://openlit.io/)):
- All-in-one GenAI observability
- Unified traces and metrics
- Cost analysis and optimization
- Open-source alternative to commercial APMs

### Emerging Standards

**AI Agent Semantic Conventions** ([OpenTelemetry Blog](https://opentelemetry.io/blog/2025/ai-agent-observability/)):
- Based on Google's AI agent white paper
- Standardizing conventions for:
  - Agent types (reactive, planning, multi-agent)
  - Tool invocations
  - Decision traces
  - Framework integration (CrewAI, AutoGen, LangGraph)

**Common Patterns**:
1. **Correlation IDs**: Track requests across subagents
2. **Hierarchical Tracing**: Parent-child relationships
3. **Cost Attribution**: Which agent/tool costs most
4. **Performance Baselines**: Compare against historical data
5. **Anomaly Detection**: Alert on unusual patterns

---

## What We Can Build

### Immediate: Commit-Agent Metrics Library

**Goal**: Standalone Python library for extracting commit-agent metrics

**File**: `.claude/lib/extract_agent_metrics.py`

**API Design**:
```python
from pathlib import Path
from typing import Optional
from dataclasses import dataclass

@dataclass
class AgentMetrics:
    """Metrics extracted from agent transcript."""
    agent_id: str
    session_id: str
    timestamp: str

    # Token metrics
    total_tokens: int
    input_tokens: int
    output_tokens: int
    cache_created_tokens: int
    cache_read_tokens: int
    cache_hit_rate: float

    # Execution metrics
    duration_ms: Optional[int]
    tool_use_count: int
    tool_types: dict[str, int]  # {"Bash": 5, "Read": 2}

    # Git metrics (commit-agent specific)
    commits_created: int
    commit_hashes: list[str]
    files_changed: int
    pre_commit_iterations: int

    # Quality indicators
    phases_executed: set[str]
    logsift_errors: int
    read_own_instructions: bool

def extract_from_hook_context(context_file: Path) -> AgentMetrics:
    """
    Extract metrics using hook-provided context.

    Args:
        context_file: /tmp/claude-agent-context-{session_id}.json

    Returns:
        AgentMetrics with all available data
    """
    # Read hook context to get parent transcript path
    # Extract agentId from parent transcript
    # Read agent transcript
    # Parse and aggregate metrics
    pass

def extract_from_agent_transcript(agent_transcript: Path) -> AgentMetrics:
    """Direct extraction from known agent transcript path."""
    pass

def extract_from_parent_transcript(parent_transcript: Path,
                                   agent_id: Optional[str] = None) -> AgentMetrics:
    """Extract from parent transcript (slower, for fallback)."""
    pass
```

**Usage in commit-agent Phase 7**:
```bash
# Replace current bash metrics script
python3 /Users/chris/dotfiles/.claude/lib/extract_agent_metrics.py \
  --context-file /tmp/claude-agent-context-{session_id}.json \
  --output /Users/chris/dotfiles/.claude/metrics/commit-metrics-$(date +%Y-%m-%d).jsonl
```

**Output Format** (JSONL):
```json
{
  "timestamp": "2025-12-05T07:39:05.708Z",
  "type": "commit-agent",
  "agent_id": "b59f7ef4",
  "session_id": "dac1b79c-1435-4fa0-8d76-b4fd3a5b4a4e",
  "transcript_path": "~/.claude/projects/-Users-chris-dotfiles/agent-b59f7ef4.jsonl",
  "cwd": "/Users/chris/dotfiles",
  "tokens": {
    "total": 17604,
    "input": 5,
    "output": 59,
    "cache_created": 5479,
    "cache_read": 17387,
    "cache_hit_rate": 0.76
  },
  "execution": {
    "duration_ms": 67362,
    "tool_use_count": 10,
    "tool_types": {"Bash": 7, "Read": 0, "Edit": 0}
  },
  "git": {
    "commits_created": 1,
    "commit_hashes": ["ae9c5d8"],
    "files_changed": 2,
    "pre_commit_iterations": 1
  },
  "quality": {
    "phases_executed": ["phase_4", "phase_5", "phase_7"],
    "logsift_errors": 0,
    "read_own_instructions": false
  }
}
```

### Medium-Term: Universal Agent Metrics

**Goal**: Generic metrics extraction for any agent type

**File**: `.claude/lib/agent_metrics/`

**Structure**:
```bash
.claude/lib/agent_metrics/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ extractors/
â”‚   â”œâ”€â”€ base.py           # BaseAgentMetrics class
â”‚   â”œâ”€â”€ commit_agent.py   # Commit-specific metrics
â”‚   â”œâ”€â”€ explore_agent.py  # Explore-specific metrics
â”‚   â””â”€â”€ custom.py         # Extensible for new agents
â”œâ”€â”€ parsers/
â”‚   â”œâ”€â”€ transcript.py     # JSONL parsing
â”‚   â””â”€â”€ hooks.py          # Hook context reading
â””â”€â”€ exporters/
    â”œâ”€â”€ jsonl.py          # Current format
    â””â”€â”€ otel.py           # OpenTelemetry export
```

**Usage**:
```python
from agent_metrics import extract_metrics, CommitAgentMetrics

# Auto-detect agent type and extract appropriate metrics
metrics = extract_metrics(context_file="/tmp/claude-agent-context-xyz.json")

if isinstance(metrics, CommitAgentMetrics):
    print(f"Commits: {metrics.commits_created}")
    print(f"Cache hit rate: {metrics.cache_hit_rate:.1%}")
```

### Long-Term: Full Observability Stack

**Goal**: Claude Code + OpenTelemetry + Grafana for comprehensive monitoring

**Components**:

1. **Metrics Collection**:
   - Python library extracts from transcripts
   - Exports to both JSONL (local) and OTEL (observability platform)

2. **OpenTelemetry Exporter**:
   - Convert metrics to OTEL format
   - Send to configured endpoint
   - Spans for agent execution
   - Traces for parent-child relationships

3. **Grafana Dashboards**:
   - Token usage over time
   - Cost tracking by agent type
   - Cache efficiency
   - Performance trends
   - Quality metrics (phases executed, error rates)

4. **Alerting**:
   - Token budget exceeded
   - Abnormal execution time
   - High error rates
   - Cost anomalies

**Example OTEL Export**:
```python
from opentelemetry import metrics
from opentelemetry.exporter.otlp.proto.grpc.metric_exporter import OTLPMetricExporter
from opentelemetry.sdk.metrics import MeterProvider

# Configure exporter
exporter = OTLPMetricExporter(endpoint="http://localhost:4317")
meter_provider = MeterProvider(metric_readers=[exporter])

# Create meters
meter = meter_provider.get_meter("claude.agent.commit")

# Record metrics
token_counter = meter.create_counter("commit.agent.tokens.total")
token_counter.add(17604, {"agent_id": "b59f7ef4", "type": "commit"})

duration_histogram = meter.create_histogram("commit.agent.duration_ms")
duration_histogram.record(67362, {"agent_id": "b59f7ef4"})
```

---

## Implementation Recommendations

### Phase 1: Python Metrics Library (This Week)

**Tasks**:
1. Create `.claude/lib/extract_agent_metrics.py`
2. Implement `extract_from_hook_context()` function
3. Parse agent transcript (Option 2: agentId â†’ agent transcript)
4. Extract all available metrics
5. Output to JSONL in existing format
6. Replace bash script in commit-agent Phase 7
7. Test with multiple commit-agent runs

**Success Criteria**:
- Metrics file has all fields populated (no "unavailable")
- Accurate token counts
- Correct commit hashes and file counts
- Phase execution tracking works
- Performance: <500ms to extract metrics

**Deliverables**:
- Python script
- Updated commit-agent.md (Phase 7)
- Test cases
- Documentation in `docs/architecture/metrics-tracking.md`

### Phase 2: Analysis Enhancements (Next Week)

**Tasks**:
1. Extend `analyze-claude-metrics` to parse new fields
2. Add cache efficiency analysis
3. Compare token usage across commits
4. Identify optimization opportunities
5. Create weekly summary report

**Success Criteria**:
- Can compare cache hit rates over time
- Can identify high-token commits
- Can track quality metrics (phases, logsift)

### Phase 3: OpenTelemetry Export (Future)

**Tasks**:
1. Enable built-in Claude Code telemetry
2. Set up local observability stack (Grafana + Prometheus)
3. Create Python OTEL exporter for agent metrics
4. Build Grafana dashboards
5. Configure alerts

**Success Criteria**:
- Real-time metrics visible in Grafana
- Historical trends tracked
- Alerts fire on anomalies
- Cost tracking accurate

---

## Tradeoffs and Decisions

### Parent Transcript vs Agent Transcript

| Factor | Parent Transcript | Agent Transcript |
|--------|------------------|------------------|
| **Size** | Large (~90k tokens) | Small (~30 lines) |
| **Access** | Direct from hook | Need to extract agentId first |
| **Content** | All agents + parent | Single agent only |
| **Performance** | Slower (large file) | Faster (small file) |
| **Reliability** | Always exists | Should exist for Task agents |
| **Context** | Includes parent relationship | Isolated |

**Decision**: Use **Agent Transcript** (Option 2)
- Better performance (small file)
- Cleaner separation
- Scalable to many agents
- Worth the extra step to extract agentId

### Python vs Bash

| Factor | Python | Bash |
|--------|--------|------|
| **JSON Parsing** | Native, structured | jq required |
| **Data Structures** | Dictionaries, classes | String manipulation |
| **Error Handling** | try/except, typed | Return codes, brittle |
| **Testing** | pytest, mocking | bats, harder to test |
| **Maintainability** | Easier to extend | Gets messy with complexity |
| **Integration** | Subprocess for bash | Native in shell |

**Decision**: **Python** for metrics extraction
- Complex aggregations needed
- Better long-term maintainability
- Type safety prevents bugs
- Can still shell out to bash for specific ops

### Local JSONL vs OpenTelemetry

| Factor | Local JSONL | OpenTelemetry |
|--------|-------------|---------------|
| **Setup** | None (already working) | Infrastructure needed |
| **Privacy** | Fully local | Depends on endpoint |
| **Analysis** | Manual with jq/Python | Rich dashboards |
| **Retention** | Manual management | Configurable |
| **Cost** | Free | Depends on platform |
| **Real-time** | No | Yes |

**Decision**: **Both** (phased approach)
- Phase 1: JSONL (immediate value)
- Phase 2: Add OTEL export (optional)
- Users can choose based on needs

### Comprehensive vs Minimal Metrics

| Approach | Comprehensive | Minimal |
|----------|--------------|---------|
| **Fields** | 20+ metrics | 5-8 core metrics |
| **Parsing Time** | Slower | Faster |
| **Usefulness** | Deep analysis | Quick overview |
| **Complexity** | Higher | Lower |
| **Future Value** | High | Limited |

**Decision**: **Comprehensive** from the start
- Storage is cheap (~1KB per entry)
- Parsing once is better than multiple times
- Can always filter to minimal view later
- Future analysis will need the data

---

## Next Steps

### Immediate (This Session)

1. âœ… Research transcript structure
2. âœ… Compare parent vs agent transcripts
3. âœ… Investigate OpenTelemetry options
4. âœ… Create this planning document
5. â³ Implement Python metrics extraction library
6. â³ Test with commit-agent
7. â³ Update documentation

### This Week

- Replace bash metrics script with Python
- Validate metrics accuracy
- Run multiple commits to test
- Document findings in learnings/

### Next Week

- Enhance analysis script
- Create visualization examples
- Test with other agent types (if applicable)

### Future

- OpenTelemetry export implementation
- Grafana dashboard setup
- Cost tracking integration
- Automated quality detection

---

## Open Questions

1. **Should we extract metrics in real-time or post-hoc?**
   - Real-time: During agent execution (Phase 7)
   - Post-hoc: Batch analysis of historical transcripts
   - **Leaning**: Real-time in Phase 7, with option for historical analysis

2. **How to handle partial/interrupted agent runs?**
   - Agent crashed mid-execution
   - Transcript incomplete
   - **Idea**: Mark as "partial" in metrics, include duration up to interruption

3. **Should metrics be per-commit or per-agent-invocation?**
   - Per-commit: One metric entry per commit created
   - Per-invocation: One entry for the entire agent run (could create multiple commits)
   - **Leaning**: Per-invocation, with commits array

4. **How to correlate metrics across sessions?**
   - Same task, different sessions
   - Compare A/B test results
   - **Idea**: Add optional `task_type` or `tags` field

5. **What about non-Task agents?**
   - Agents invoked via SDK
   - Interactive sessions
   - **Idea**: Generic fallback to parent transcript parsing

---

## References

### Research Sources

**Claude Code Observability**:
- [SigNoz - Claude Code Monitoring with OpenTelemetry](https://signoz.io/blog/claude-code-monitoring-with-opentelemetry/)
- [Official Monitoring Docs](https://docs.claude.com/en/docs/claude-code/monitoring-usage)
- [Honeycomb - Can Claude Code Observe Its Own Code?](https://www.honeycomb.io/blog/can-claude-code-observe-its-own-code)
- [Grafana Cloud Guide](https://quesma.com/blog/track-claude-code-usage-and-limits-with-grafana-cloud/)

**Agent SDK Best Practices**:
- [Building Agents with Claude Agent SDK](https://www.anthropic.com/engineering/building-agents-with-the-claude-agent-sdk)
- [Dev-Agent-Lens for Observability](https://arize.com/blog/claude-code-observability-and-tracing-introducing-dev-agent-lens/)
- [Monitoring Claude AI Agents](https://medium.com/@spacholski99/monitoring-claude-ai-agents-why-standard-tools-dont-work-and-what-does-2c2c489135c9)

**LLM Observability Standards**:
- [OpenTelemetry LLM Observability](https://opentelemetry.io/blog/2024/llm-observability/)
- [OpenLLMetry by Traceloop](https://www.traceloop.com/blog/openllmetry)
- [Grafana LLM Observability Guide](https://grafana.com/blog/2024/07/18/a-complete-guide-to-llm-observability-with-opentelemetry-and-grafana-cloud/)
- [AI Agent Observability Standards](https://opentelemetry.io/blog/2025/ai-agent-observability/)

**Tools and Libraries**:
- [claude-code-otel (GitHub)](https://github.com/ColeMurray/claude-code-otel)
- [claude_telemetry (GitHub)](https://github.com/TechNickAI/claude_telemetry)
- [OpenLLMetry (GitHub)](https://github.com/traceloop/openllmetry)
- [OpenLIT](https://openlit.io/)
- [Langfuse OpenTelemetry](https://langfuse.com/integrations/native/opentelemetry)

### Internal Documentation

- `docs/architecture/metrics-tracking.md` - Current metrics system
- `docs/claude-code/transcript-access.md` - Transcript access patterns
- `docs/research/ai/context-engineering.md` - Token optimization
- `.claude/lib/log-commit-metrics.sh` - Current bash implementation

---

## Conclusion

We have a clear path forward:

1. **Immediate**: Build Python metrics extraction library using agent transcripts
2. **Near-term**: Replace bash script, validate accuracy, document learnings
3. **Medium-term**: Extend to other agent types, enhance analysis
4. **Long-term**: OpenTelemetry export, Grafana dashboards, alerting

The **agent transcript approach** (Option 2) is the right choice:
- Smaller files, faster parsing
- Clean separation of concerns
- Concurrency-safe via hook-provided parent transcript
- Scalable to multiple agents

**Python** is the right tool:
- Complex aggregations
- Type safety
- Better testing
- Long-term maintainability

Next action: Implement `.claude/lib/extract_agent_metrics.py` and test it with commit-agent.
