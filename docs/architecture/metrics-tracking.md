# Metrics Tracking Architecture

Technical documentation for the unified Claude Code workflow metrics system.

!!! info "User Guide"
    For usage instructions, see [Working with Claude Code](../claude-code/working-with-claude.md#metrics-quality-tracking)

## Overview

The unified metrics tracking system provides quantitative and qualitative assessment of all Claude Code workflows (commit agent, logsift commands, future tools), enabling data-driven optimization and performance monitoring.

### Design Goals

1. **Zero-friction collection**: Metrics gathered automatically without user intervention
2. **Actionable insights**: Data structured for comparative analysis and debugging
3. **Quality over quantity**: Track correctness and methodology, not just token counts
4. **Lightweight**: Minimal performance impact, fail-safe operation
5. **Complete visibility**: Full transcripts captured for detailed post-session analysis

### Components

```mermaid
graph TD
    A[User requests commit] --> B[Main agent invokes commit-agent]
    B --> C[Commit agent creates commits]
    C --> D[Commit agent Phase 7: Self-report metrics]
    D --> E[.claude/metrics/command-metrics-*.jsonl]

    F[User runs /logsift] --> G[SlashCommand tool executes]
    G --> H[Logsift monitors command]
    H --> I[PostToolUse: track-slash-command-metrics]
    I --> E

    E --> J[analyze-claude-metrics]
    J --> K[Summary reports by type]
    J --> L[Detailed command history]

    E --> M[quality-log.md]
    M --> N[Manual quality assessment]
```

## Data Model

### Unified JSONL Format

**File**: `.claude/metrics/command-metrics-YYYY-MM-DD.jsonl`

**Format**: JSON Lines (one JSON object per line, all workflow types in same file)

**Why JSONL?**

- Append-only (no need to parse entire file)
- One workflow = one line = atomic operation
- Easy to process with Python, `jq`, `grep`, streaming parsers
- Resilient to corruption (only last line at risk)
- Chronological view of all activity

### Metric Types

**Base schema** (all entries):

```json
{
  "timestamp": "2025-12-04T20:15:30.123456",
  "session_id": "abc123def456",
  "type": "commit-agent|logsift|logsift-auto",
  "cwd": "/Users/chris/dotfiles"
}
```

**Commit Agent Metrics**:

```json
{
  "timestamp": "2025-12-04T20:15:30",
  "session_id": "abc123",
  "type": "commit-agent",
  "cwd": "/Users/chris/dotfiles",
  "commits_created": 1,
  "commit_hashes": ["774eb33"],
  "files_committed": 7,
  "files_renamed": 5,
  "files_modified": 2,
  "files_created": 1,
  "pre_commit_iterations": 1,
  "pre_commit_failures": 0,
  "tokens_used": 19600,
  "tool_uses": 9,
  "phase_4_executed": true,
  "phase_5_executed": true,
  "phase_5_logsift_errors": 0,
  "read_own_instructions": false,
  "main_agent_overhead_tokens": 552,
  "duration_seconds": 54.2
}
```

**Logsift Metrics**:

```json
{
  "timestamp": "2025-12-04T20:18:00",
  "session_id": "abc123",
  "type": "logsift",
  "cwd": "/Users/chris/dotfiles",
  "command": "/logsift",
  "full_command": "/logsift \"bash test.sh\" 15",
  "underlying_command": "bash test.sh",
  "timeout_minutes": 15,
  "duration_seconds": 125.3,
  "exit_code": 0,
  "errors_found": 5,
  "warnings_found": 2,
  "log_file": "/Users/chris/.cache/logsift/raw/2025-12-04T20:18:00-bash-test.sh.log"
}
```

**Logsift-Auto Metrics**:

```json
{
  "timestamp": "2025-12-04T20:20:00",
  "session_id": "abc123",
  "type": "logsift-auto",
  "cwd": "/Users/chris/dotfiles",
  "command": "/logsift-auto",
  "natural_language_input": "run the test install script",
  "interpreted_command": "bash management/test-install.sh",
  "parsing_successful": true,
  "duration_seconds": 180.5,
  "exit_code": 0,
  "errors_found": 3,
  "warnings_found": 1,
  "log_file": "/Users/chris/.cache/logsift/raw/..."
}
```

### Logsift Analysis Data

**File**: `~/.local/share/logsift/logs/session-*.json`

**Generated by**: Logsift itself

```json
{
  "command": "bash test-install.sh",
  "duration": "12m 34s",
  "exit_code": 1,
  "analysis": {
    "error_count": 5,
    "warning_count": 3,
    "errors": [
      {"line": 142, "message": "Package 'foo' not found"},
      {"line": 256, "message": "Connection refused"}
    ],
    "warnings": [
      {"line": 89, "message": "Deprecated flag --old-style"}
    ]
  }
}
```

**Usage**: Referenced by `analyze-claude-metrics` for error/warning counts

### Manual Quality Log

**File**: `.claude/metrics/quality-log.md`

**Format**: Markdown with structured entries

```markdown
## YYYY-MM-DD HH:MM - Session ID

**Command**: `/logsift "command"`

**Context**: Brief description

**Quantitative**:
- Initial errors: X
- Final errors: 0
- Iterations: Y
- Tokens: Z (from /cost)

**Qualitative**:
- Correctness: ✅/⚠️/❌
- Efficiency: ✅/⚠️/❌
- Methodology: ✅/⚠️/❌

**Notes**:
- Observations
- What worked well
- What could improve

**Comparison** (if applicable):
- /logsift vs /logsift-auto differences
```

**Purpose**: Capture qualitative assessments that can't be automated

## Implementation

### Collection Methods

**1. Commit Agent Self-Reporting** (Phase 7):

- **File**: `.claude/agents/commit-agent.md` (Phase 7)
- **Helper**: `.claude/lib/commit-agent-metrics.py`
- **Trigger**: After commits created, before response to main agent
- **Advantages**: Most accurate (knows exact tokens, iterations, phases)

**2. PostToolUse Hook for Slash Commands**:

- **File**: `.claude/hooks/track-slash-command-metrics`
- **Language**: Python 3
- **Trigger**: PostToolUse hook (after SlashCommand tool completes)
- **Targets**: `/logsift` and `/logsift-auto` commands

### PostToolUse Hook Logic

1. Read hook input from stdin (JSON)
2. Check if tool_name == "SlashCommand"
3. Extract command from tool_input
4. Filter for /logsift commands (skip others)
5. Create metric entry with timestamp
6. Append to daily JSONL file

**Error handling**: Never blocks - all exceptions caught and logged to stderr

**Configuration**: None required (uses git repo root + `.claude/metrics/`)

### Analysis Tool: analyze-claude-metrics

**File**: `apps/common/analyze-claude-metrics`

**Language**: Pure Python 3 (stdlib only)

**Architecture**:

- `MetricsAnalyzer` class for loading and analyzing JSONL
- `CommitAgentMetrics` and `LogsiftMetrics` dataclasses
- Type hints and proper separation of concerns
- argparse CLI with colored ANSI output

**Features**:

- Summary mode: Breakdown by workflow type with key metrics
- Detailed mode: Recent command history with timestamps
- Type filtering: `--type commit-agent|logsift|logsift-auto`
- Date filtering: `--date YYYY-MM-DD`
- Commit agent stats: tokens, phases, files, iterations
- Logsift stats: success rate, errors/warnings found

**Usage**:

```bash
analyze-claude-metrics                    # Summary of all workflows
analyze-claude-metrics --type commit-agent # Only commit agent
analyze-claude-metrics --date 2025-12-04  # Specific date
analyze-claude-metrics --detailed         # With recent commands
```

**Output**:

- Total commands tracked
- Breakdown by type (logsift, logsift-auto, commit-agent)
- Type-specific metrics (varies by workflow)
- Recent commands (detailed mode only)

## Key Performance Indicators

### Quality Metrics (Manual Assessment)

**Success Rate**:

```text
Success Rate = (Successful Sessions / Total Sessions) * 100
```

Where "successful" = all errors resolved, tests passing

**Root Cause Accuracy**:

```text
RCA Accuracy = (Correct Root Cause Identifications / Total Sessions with Related Errors) * 100
```

Requires manual assessment: Did Claude correctly identify the root cause?

**Methodology Compliance**:

```text
Compliance = (Sessions Following 5-Phase Approach / Total Sessions) * 100
```

Check quality log for:

- Did Claude wait for full analysis?
- Did Claude determine error relationships?
- Did Claude read files before editing?
- Did Claude iterate properly?

### Efficiency Metrics (Semi-Automated)

**Average Iterations**:

```text
Avg Iterations = Sum(Iterations per Session) / Total Sessions
```

Track in quality log: How many logsift runs until success?

**Token Usage per Error**:

```text
Tokens per Error = Total Tokens / Total Errors Resolved
```

Requires `/cost` data + error counts from logsift

**Context Efficiency**:

```text
Context Saved = (Original Output Lines - Logsift Filtered Lines) / Original Output Lines * 100
```

Typical: 95-98% reduction (10,000 lines → 200 lines)

### Comparative Metrics

**Success Delta**:

```text
Delta = (Success Rate of /logsift-auto) - (Success Rate of /logsift)
```

Positive = /logsift-auto more successful
Negative = /logsift more successful

**Token Delta**:

```text
Delta = Avg Tokens(/logsift-auto) - Avg Tokens(/logsift)
```

Hypothesis: /logsift-auto uses slightly more tokens for command interpretation

**Parsing Accuracy** (/logsift-auto only):

```text
Parsing Accuracy = (Correctly Parsed Commands / Total /logsift-auto Invocations) * 100
```

Manual assessment: Did Claude run the right command?

## Data Retention

### Automated Logs

**Command metrics**: `.claude/metrics/command-metrics-*.jsonl`

- Retention: Indefinite (lightweight, ~1KB per day)
- Rotation: New file per day
- Archive: Move to `.claude/metrics/archive/YYYY/` annually

**Logsift logs**: `~/.local/share/logsift/logs/*.json`

- Retention: Managed by logsift
- Typically: Last 100 sessions
- Purge: `logsift clean --older-than 30d`

### Manual Logs

**Quality log**: `.claude/metrics/quality-log.md`

- Retention: Indefinite
- Growth: ~200 bytes per entry
- Archive: Split annually to `quality-log-YYYY.md` if >100 entries

## Privacy & Security

**What's tracked**:

- Command invocations and types
- Timestamps and session IDs
- Working directories
- Error/warning counts

**What's NOT tracked**:

- Actual command output or errors (those are in logsift logs)
- File contents or code changes
- API keys or credentials
- Personal identifiable information

**Data location**:

- All data stored locally in dotfiles repo
- No external transmission
- Included in `.gitignore` if contains sensitive paths

**OpenTelemetry export** (optional):

- Controlled by environment variables
- Requires explicit enablement
- Sends to configured OTEL endpoint only

## Future Enhancements

### Automated Quality Detection

Parse transcripts to detect methodology compliance:

```python
# Pseudo-code
def check_methodology(transcript):
    checks = {
        "waited_for_analysis": search_for("logsift analysis complete"),
        "determined_relationships": search_for("related|independent"),
        "read_before_edit": check_tool_sequence(["Read", "Edit"]),
        "iterated": count_logsift_invocations() > 1
    }
    return ComplianceScore(checks)
```

### Cost Tracking Integration

Integrate with Anthropic Admin API:

```bash
# Fetch usage for specific session
curl -H "x-api-key: $ADMIN_API_KEY" \
  https://api.anthropic.com/v1/organizations/usage_report/messages \
  | jq '.message_usage[] | select(.session_id == "abc123")'
```

Requires: Admin API key (organization admins only)

### Comparison Dashboard

Generate visual comparison report:

```markdown
# Weekly Report: 2025-12-01 to 2025-12-07

## Usage
| Command | Invocations | Avg Duration |
|---------|------------|--------------|
| /logsift | 15 | 8m 32s |
| /logsift-auto | 8 | 9m 15s |

## Success Rate
- /logsift: 87% (13/15)
- /logsift-auto: 75% (6/8)

## Token Efficiency
- /logsift: 38k avg tokens
- /logsift-auto: 42k avg tokens (+10%)

## Recommendation
Continue using /logsift for known commands.
Use /logsift-auto for exploratory testing.
```

### Pre-commit Agent Metrics

Extend to track commit workflows:

```json
{
  "timestamp": "2025-12-03T16:00:00",
  "type": "commit-agent",
  "pre_commit_hooks": ["shellcheck", "markdownlint", "prettier"],
  "iterations": 2,
  "initial_errors": 5,
  "final_errors": 0,
  "tokens_used": 12000
}
```

Track:

- Pre-commit iterations (how many fix-and-retry cycles)
- Hook failures by type
- Token savings from logsift filtering hook output
- Commit message quality scores

## Testing

### Unit Tests (Future)

```bash
# Test metric collection
python -m pytest .claude/hooks/test_track_command_metrics.py

# Test analysis script
bash tests/test_analyze_logsift_metrics.sh
```

### Integration Tests

```bash
# Simulate command invocation
echo '{"session_id": "test123", "cwd": "/tmp", "command": "/logsift"}' | \
  .claude/hooks/track-slash-command-metrics

# Verify file created
test -f .claude/metrics/command-metrics-$(date +%Y-%m-%d).jsonl

# Verify content
jq . < .claude/metrics/command-metrics-$(date +%Y-%m-%d).jsonl
```

## References

### External

- [Langfuse Token Tracking](https://langfuse.com/docs/observability/features/token-and-cost-tracking) - LLM cost tracking patterns
- [Datadog LLM Observability](https://www.datadoghq.com/product/llm-observability/) - Production monitoring
- [Sentry KPIs](https://blog.sentry.io/core-kpis-llm-performance-how-to-track-metrics/) - Core performance indicators
- [Claude Code Monitoring](https://code.claude.com/docs/en/monitoring-usage.md) - Official usage tracking

### Internal

- [Working with Claude Code](../claude-code/working-with-claude.md) - User guide
- [Structured Logging](./structured-logging.md) - Log format standards
- [Shell Libraries](./shell-libraries.md) - Logging implementations
- [Hooks Reference](../reference/tools/hooks.md) - Available hooks

## Changelog

### 2025-12-04 - Unified Metrics System

- Created `track-slash-command-metrics` PostToolUse hook
- Implemented `analyze-claude-metrics` script (pure Python)
- Added commit agent self-reporting (Phase 7)
- Defined unified JSONL schema for all workflow types
- Created quality log template
- Documented KPIs and analysis methodology
